{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model+grid_search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VubK6qZQUmeM",
        "colab_type": "text"
      },
      "source": [
        "# Challenges in Computational Linguistics, WS 19/20\n",
        "\n",
        "Blaschke Verena, ISCL MA<br/>\n",
        "Korniyenko Maxim, ISCL MA<br/>\n",
        "Tureski Sam, ML MA<br/>\n",
        "\n",
        "-----\n",
        "## SemEval2020-11: Propaganda Detection\n",
        "### Task 1: Span identification\n",
        "-----\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-5HfEvv3STwo",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_1W_TMlZXll",
        "colab_type": "text"
      },
      "source": [
        "# model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uUPU9_5SZ9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import takewhile\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from keras.layers import Bidirectional, CuDNNLSTM, Dense, Dropout, \\\n",
        "    TimeDistributed, Activation\n",
        "from keras.models import Sequential\n",
        "from sklearn import svm, preprocessing\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "########################\n",
        "# Processing the input #\n",
        "########################\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def get_comments(filename, url=True):\n",
        "    if url:\n",
        "        comments = []\n",
        "        with urllib.request.urlopen(filename) as f:\n",
        "            for line in f:\n",
        "                if line.startswith(b'#'):\n",
        "                    comments.append(line.decode(\"utf-8\"))\n",
        "                else:\n",
        "                    break\n",
        "        return comments\n",
        "    with open(filename, 'r', encoding='utf8') as f:\n",
        "        commentiter = takewhile(lambda s: s.startswith('#'), f)\n",
        "        comments = list(commentiter)\n",
        "    return comments\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def get_cols(input_df, col):\n",
        "    return input_df.groupby('sent_id')[col].apply(list).to_frame()\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def add_sent_lens(input_df, col='token'):\n",
        "    input_df['n_toks'] = input_df[col].apply(lambda x: len(x))\n",
        "    return input_df\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def get_features(input_df, feature_cols):\n",
        "    x = add_sent_lens(get_cols(input_df, 'token'))\n",
        "    for feature in feature_cols:\n",
        "        x = pd.merge(left=x, right=get_cols(input_df, feature),\n",
        "                     left_on='sent_id', right_on='sent_id')\n",
        "    return x\n",
        "\n",
        "\n",
        "# Helper method for encode_x_bert\n",
        "def bert_embeddings_for_sent(bert_tokens, row, feature_header, embedding_matrix,\n",
        "                             embed_dim, sent_idx, uncased):\n",
        "    if len(bert_tokens) < len(row.token):\n",
        "        print('BERT', [i[0] for i in bert_tokens])\n",
        "        print('X', row.token)\n",
        "    word_idx = 0\n",
        "    for (tok, embed) in bert_tokens:\n",
        "        if word_idx == row.n_toks:\n",
        "            break\n",
        "        word = str(row.token[word_idx])\n",
        "        if word == '\\ufeff':  # Prints a warning, but is dealt with.\n",
        "            word_idx += 1\n",
        "            continue\n",
        "        if uncased:\n",
        "            word = word.lower()\n",
        "        if tok == word or word.startswith(tok):\n",
        "            # startswith: Use embedding of first subtoken\n",
        "            embedding_matrix[sent_idx - 1][word_idx][:embed_dim] = embed\n",
        "            for i, feature in enumerate(feature_header):\n",
        "                embedding_matrix[sent_idx - 1][word_idx][embed_dim + i] = \\\n",
        "                    getattr(row, feature)[word_idx]\n",
        "            word_idx += 1\n",
        "            continue\n",
        "        if tok.startswith('##') and not word.startswith('##'):\n",
        "            # BERT word continutation prefix (e.g. per ##pet ##uate)\n",
        "            continue\n",
        "\n",
        "\n",
        "# Task 1: Token embeddings\n",
        "def encode_x_bert(x, bert_file, feature_header, max_seq_len, embed_dim=768,\n",
        "                  uncased=True):\n",
        "    # TODO this currently assumes that the BERT file only contains information\n",
        "    # about a single layer. extend this to multiple layers?\n",
        "    embedding_matrix = np.zeros([len(x), max_seq_len,\n",
        "                                 embed_dim + len(feature_header)])\n",
        "    prev_sent_idx = 1\n",
        "    bert_tokens = []\n",
        "    sentences = x.itertuples()\n",
        "    with open(bert_file, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            cells = line.split('\\t')\n",
        "            sent_idx = int(cells[0])\n",
        "            layer = int(cells[1])\n",
        "            token = cells[2]\n",
        "            embedding = np.fromstring(cells[3][1:-1], sep=',')\n",
        "\n",
        "            if sent_idx != prev_sent_idx:\n",
        "                if sent_idx % 1000 == 0:\n",
        "                    print(\"BERT embeddings for sentence\", sent_idx)\n",
        "                row = next(sentences)\n",
        "                assert row.Index == prev_sent_idx\n",
        "                bert_embeddings_for_sent(bert_tokens, row, feature_header,\n",
        "                                         embedding_matrix, embed_dim,\n",
        "                                         prev_sent_idx, uncased)\n",
        "                bert_tokens = []\n",
        "\n",
        "            bert_tokens.append((token, embedding))\n",
        "            prev_sent_idx = sent_idx\n",
        "\n",
        "    # Last line:\n",
        "    row = next(sentences)\n",
        "    bert_embeddings_for_sent(bert_tokens, row, feature_header, embedding_matrix,\n",
        "                             embed_dim, prev_sent_idx, uncased)\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "# Task 2: Sequence embeddings\n",
        "def encode_x_seq(x, bert_file, feature_header, embed_dim=768, uncased=True,\n",
        "                 n_bert_layers=1):\n",
        "    embedding_matrix = np.zeros([len(x),\n",
        "                                 embed_dim * n_bert_layers + len(feature_header)])\n",
        "    prev_sent_idx = 1\n",
        "    bert_tokens = []\n",
        "    sequences = x.itertuples()\n",
        "    with open(bert_file, encoding='utf8') as f:\n",
        "        idx = 0\n",
        "        for line in f:\n",
        "            row = next(sequences)\n",
        "            for bert_layer in range(n_bert_layers):\n",
        "                cells = line.split('\\t')\n",
        "                sent_idx = int(cells[0])\n",
        "                layer = cells[1]\n",
        "                seq = cells[2]\n",
        "                print(cells)\n",
        "                print(line)\n",
        "                print(bert_file)\n",
        "                print(cells[3])\n",
        "                embedding = np.fromstring(cells[3][1:-1], sep=',')\n",
        "                text = row.text\n",
        "                if uncased:\n",
        "                    text = text.lower()\n",
        "                # assert text == seq or text + ' ' + text == seq\n",
        "                embedding_matrix[idx][embed_dim * bert_layer:embed_dim * (bert_layer + 1)] = embedding\n",
        "                if n_bert_layers > 1 and bert_layer < n_bert_layers - 1:\n",
        "                    line = next(f)\n",
        "            for i, feature in enumerate(feature_header):\n",
        "                embedding_matrix[idx][embed_dim * n_bert_layers + i] = getattr(row, feature)\n",
        "            idx += 1\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def encode_x(x, word2embedding, feature_header, max_seq_len,\n",
        "             embed_dim, uncased):\n",
        "    \"\"\"Encode the input data.\n",
        "\n",
        "    Arguments:\n",
        "    x -- a Pandas dataframe\n",
        "    word2embedding -- a dict(str -> np.array) from tokens to embeddings\n",
        "    feature_header -- dataframe names of additional feature columns\n",
        "    max_seq_len -- the maximum number of tokens per sentence in x\n",
        "    embed_dim -- the array length of the vectors in word2embedding\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros([len(x), max_seq_len,\n",
        "                                 embed_dim + len(feature_header)])\n",
        "    for row in x.itertuples():\n",
        "        sent_idx = row.Index - 1\n",
        "        for tok_idx in range(row.n_toks):\n",
        "            word = str(row.token[tok_idx])\n",
        "            if uncased:\n",
        "                word = word.lower()\n",
        "            embedding_matrix[sent_idx][tok_idx][:embed_dim] = \\\n",
        "                word2embedding.get(word, np.random.randn(embed_dim))\n",
        "            for i, feature in enumerate(feature_header):\n",
        "                embedding_matrix[sent_idx][tok_idx][embed_dim + i] = \\\n",
        "                    getattr(row, feature)[tok_idx]\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def encode_y(y, label2idx, max_seq_len, n_classes):\n",
        "    if n_classes == 1:\n",
        "        if max_seq_len > 1:\n",
        "            labels = np.zeros([len(y), max_seq_len])\n",
        "        else:\n",
        "            labels = np.zeros(len(y))\n",
        "    else:\n",
        "        labels = np.zeros([len(y), max_seq_len, n_classes])\n",
        "\n",
        "    if max_seq_len > 1:\n",
        "        for row in y.itertuples():\n",
        "            sent_idx = row.Index - 1\n",
        "            for tok_idx, label in enumerate(row.label):\n",
        "                labels[sent_idx][tok_idx] = label2idx[label]\n",
        "    else:\n",
        "        for row in y.iteritems():\n",
        "            labels[row[0]] = label2idx[row[1]]\n",
        "    return labels\n",
        "\n",
        "\n",
        "def prepare_data(config, word2embedding, training):\n",
        "    # We're getting the comments this way so we can:\n",
        "    # - add them to the output\n",
        "    # - parse lines that actually contain '#' as token\n",
        "    if training:\n",
        "        infile = config.TRAIN_URL\n",
        "    else:\n",
        "        infile = config.DEV_URL\n",
        "    comments = get_comments(infile, config.ONLINE_SOURCES)\n",
        "    df = pd.read_csv(infile, sep='\\t', skiprows=len(comments), quoting=3,\n",
        "                     encoding='utf8')\n",
        "    \n",
        "    if config.TOKEN_LVL:\n",
        "        std_cols = ['document_id', 'sent_id', 'token_start',\n",
        "                    'token_end', 'token', 'label']\n",
        "    else:\n",
        "        std_cols = ['document_id', 'span_start', 'span_end', 'text', 'label']\n",
        "    feature_cols = []\n",
        "    for col in df.columns:\n",
        "        if config.FEATURES is None:  # Determine features based on file header\n",
        "            if col not in std_cols and col not in config.EXCLUDE_FEATURES:\n",
        "                feature_cols.append(col)\n",
        "        else:\n",
        "            if col in config.FEATURES:\n",
        "                feature_cols.append(col)\n",
        "\n",
        "    if config.TOKEN_LVL:\n",
        "        x_raw = get_features(df, feature_cols)\n",
        "    else:\n",
        "        x_raw = df\n",
        "\n",
        "    if config.USE_BERT:\n",
        "        if training:\n",
        "            bert_file = config.TRAIN_BERT\n",
        "        else:\n",
        "            bert_file = config.DEV_BERT\n",
        "        if config.TOKEN_LVL:\n",
        "            x_enc = encode_x_bert(x_raw, bert_file, feature_cols,\n",
        "                                  config.MAX_SEQ_LEN, config.EMBED_DIM,\n",
        "                                  config.UNCASED)\n",
        "        else:\n",
        "            x_enc = encode_x_seq(x_raw, bert_file, feature_cols, \n",
        "                                 config.EMBED_DIM, config.UNCASED,\n",
        "                                 config.N_BERT_LAYERS)\n",
        "    else:\n",
        "        x_enc = encode_x(x_raw, word2embedding, feature_cols,\n",
        "                     config.MAX_SEQ_LEN, config.EMBED_DIM, config.UNCASED)\n",
        "        \n",
        "    \n",
        "    print(x_enc.shape)\n",
        "\n",
        "    y = None\n",
        "    sample_weight = None\n",
        "    if training:\n",
        "        if config.TOKEN_LVL:\n",
        "            y_raw = get_cols(df, 'label')\n",
        "            if config.N_CLASSES == 3:\n",
        "                label2idx = {\"O\": [1, 0, 0], \"B\": [0, 0, 1], \"I\": [0, 1, 0]}\n",
        "            elif config.N_CLASSES == 2:\n",
        "                label2idx = {\"O\": [1, 0], \"B\": [0, 1], \"I\": [0, 1]}\n",
        "            y = encode_y(y_raw, label2idx, config.MAX_SEQ_LEN, config.N_CLASSES)\n",
        "            sample_weight = encode_y(y_raw, config.CLASS_WEIGHTS,\n",
        "                                     config.MAX_SEQ_LEN, n_classes=1)\n",
        "        else:\n",
        "            y = df.label\n",
        "            if config.CLASS_WEIGHTS:\n",
        "                sample_weight = encode_y(y, config.CLASS_WEIGHTS,\n",
        "                                         config.MAX_SEQ_LEN, n_classes=1)\n",
        "\n",
        "    return df, x_raw, x_enc, y, sample_weight, comments, feature_cols\n",
        "\n",
        "\n",
        "def load_zipped_embeddings(infile):\n",
        "    word2embedding = {}\n",
        "    with zipfile.ZipFile(infile) as f_in_zip:\n",
        "        file_in = f_in_zip.filelist[0].filename\n",
        "        i = 0\n",
        "        with f_in_zip.open(file_in, 'r') as f_in:\n",
        "            for line in f_in:\n",
        "                values = line.decode().rstrip().split()\n",
        "                word2embedding[values[0]] = np.asarray(values[1:],\n",
        "                                                       dtype='float32')\n",
        "                i += 1\n",
        "                if i % 100000 == 0:\n",
        "                    print(\"Read \" + str(i) + \" embeddings\")\n",
        "    return word2embedding\n",
        "\n",
        "\n",
        "def get_data(config, word2embedding=None):\n",
        "    if (not word2embedding) and (not config.USE_BERT):\n",
        "        if config.EMBEDDING_PATH[-4:] == '.zip':\n",
        "            word2embedding = load_zipped_embeddings(config.EMBEDDING_PATH)\n",
        "        else:\n",
        "            word2embedding = {}\n",
        "            f = open(config.EMBEDDING_PATH)\n",
        "            for line in f:\n",
        "                values = line.rstrip().split()\n",
        "                word2embedding[values[0]] = np.asarray(values[1:],\n",
        "                                                       dtype='float32')\n",
        "            f.close()\n",
        "\n",
        "    _, _, train_x, train_y, sample_weight, comments, features = prepare_data(\n",
        "        config, word2embedding, training=True)\n",
        "    dev_df, dev_raw, dev_x, _, _, _, _ = prepare_data(config, word2embedding,\n",
        "                                                   training=False)\n",
        "    return Data(train_x, train_y, sample_weight, comments,\n",
        "                dev_df, dev_raw, dev_x, features)\n",
        "\n",
        "\n",
        "class Data:\n",
        "    def __init__(self,\n",
        "                 # If initializing on the fly:\n",
        "                 train_x=None, train_y=None, sample_weight=None,\n",
        "                 comments=None, dev_df=None, dev_raw=None, dev_x=None,\n",
        "                 features=None,\n",
        "                 # If initializing from files:\n",
        "                 path=None):\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.sample_weight = sample_weight\n",
        "        self.comments = comments\n",
        "        self.features = features\n",
        "        self.dev_df = dev_df\n",
        "        self.dev_raw = dev_raw\n",
        "        self.dev_x = dev_x\n",
        "        if path:\n",
        "            self.load(path)\n",
        "\n",
        "\n",
        "    def save(self, path='gdrive/My Drive/colab_projects/data/data/'):\n",
        "        np.save(path + 'train_x', self.train_x)\n",
        "        np.save(path + 'train_y', self.train_y)\n",
        "        np.save(path + 'dev_x', self.dev_x)\n",
        "        np.save(path + 'sample_weight', self.sample_weight)\n",
        "        self.dev_raw.to_csv(path + 'dev_raw')\n",
        "        self.dev_df.to_csv(path + 'dev_df')\n",
        "        with open(path + 'comments.txt', 'w', encoding='utf8') as f:\n",
        "            for comment in self.comments:\n",
        "                f.write(comment + '\\n')\n",
        "        with open(path + 'features.txt', 'w', encoding='utf8') as f:\n",
        "            for feature in self.features:\n",
        "                f.write(feature + '\\n')\n",
        "\n",
        "\n",
        "    def load(self, path='gdrive/My Drive/colab_projects/data/data/'):\n",
        "        self.train_x = np.load(path + 'train_x.npy')\n",
        "        self.train_y = np.load(path + 'train_y.npy')\n",
        "        self.dev_x = np.load(path + 'dev_x.npy')\n",
        "        self.sample_weight = np.load(path + 'sample_weight.npy')\n",
        "        self.dev_raw = pd.read_csv(path + 'dev_raw')\n",
        "        self.dev_df = pd.read_csv(path + 'dev_df')\n",
        "        self.comments =[]\n",
        "        with open(path + 'comments.txt', 'r', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    self.comments.append(line)\n",
        "        self.features =[]\n",
        "        with open(path + 'features.txt', 'r', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    self.features.append(line)\n",
        "\n",
        "\n",
        "######################\n",
        "# Creating the model #\n",
        "######################]\n",
        "\n",
        "def get_svm(train_x, train_y):\n",
        "    model = svm.SVC(decision_function_shape='ovo')\n",
        "    model.fit(train_x, train_y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_ffnn(config, train_x, train_y, sample_weight, single_layer=False):\n",
        "    y_encoder = preprocessing.OneHotEncoder()\n",
        "    train_y_enc = y_encoder.fit_transform(train_y.to_numpy().reshape(-1, 1))\n",
        "    model = Sequential()\n",
        "    if single_layer:\n",
        "        model.add(Dense(config.N_CLASSES, activation='softmax',\n",
        "                        input_dim=train_x.shape[1]))\n",
        "    else:\n",
        "        model.add(Dense(config.HIDDEN, activation='relu',\n",
        "                        input_dim=train_x.shape[1]))\n",
        "        model.add(Dropout(config.DROPOUT))\n",
        "        model.add(Dense(config.N_CLASSES, activation='softmax'))\n",
        "    model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER,\n",
        "                  metrics=[config.METRIC])\n",
        "    history = model.fit(train_x, train_y_enc, epochs=config.EPOCHS,\n",
        "                        batch_size=config.BATCH_SIZE,\n",
        "                        sample_weight=sample_weight, verbose=1)\n",
        "    return model, history, y_encoder\n",
        "\n",
        "\n",
        "def get_bilstm(config, train_x, train_y, sample_weight):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(CuDNNLSTM(config.LSTM_UNITS,\n",
        "                                      return_sequences=True),\n",
        "                            input_shape=train_x.shape[1:]))\n",
        "    model.add(Dropout(config.DROPOUT))\n",
        "    model.add(TimeDistributed(Dense(config.N_CLASSES, activation='softmax')))\n",
        "    model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER,\n",
        "                  metrics=[config.METRIC], sample_weight_mode='temporal')\n",
        "    history = model.fit(train_x, train_y, epochs=config.EPOCHS,\n",
        "                        batch_size=config.BATCH_SIZE,\n",
        "                        sample_weight=sample_weight, verbose=1)\n",
        "    return model, history\n",
        "\n",
        "\n",
        "###############\n",
        "# Predictions #\n",
        "###############\n",
        "\n",
        "\n",
        "def get_bio_predictions(model, x, x_raw, n_classes):\n",
        "    y_hat = model.predict(x)\n",
        "    y_hat = y_hat.reshape(-1, n_classes).argmax(axis=1).reshape(x.shape[:2])\n",
        "    labels = []\n",
        "    for row in x_raw.itertuples():\n",
        "        sent_idx = row.Index - 1\n",
        "        for tok_idx in range(row.n_toks):\n",
        "            if y_hat[sent_idx][tok_idx] == 0:\n",
        "                label = \"O\"\n",
        "            elif y_hat[sent_idx][tok_idx] == 1:\n",
        "                label = \"I\"\n",
        "            else:\n",
        "                label = \"B\"\n",
        "            labels.append(label)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def si_predictions_to_spans(label_df):\n",
        "    spans = []\n",
        "    prev_label = 'O'\n",
        "    prev_span_start = '-1'\n",
        "    prev_span_end = '-1'\n",
        "    prev_article = ''\n",
        "\n",
        "    for row in label_df.itertuples():\n",
        "        article = row.document_id\n",
        "        span_start = row.token_start\n",
        "        span_end = row.token_end\n",
        "        label = row.label_pred\n",
        "\n",
        "        span, prev_span_start = update_predicted_span(article, label,\n",
        "                                                      span_start, span_end,\n",
        "                                                      prev_article, prev_label,\n",
        "                                                      prev_span_start,\n",
        "                                                      prev_span_end)\n",
        "        if span is not None:\n",
        "            spans.append(span)\n",
        "\n",
        "        prev_article = article\n",
        "        prev_label = label\n",
        "        prev_span_end = span_end\n",
        "\n",
        "    # Make sure we get the last prediction\n",
        "    span, _ = update_predicted_span(article, label, span_start, span_end,\n",
        "                                    prev_article, prev_label, prev_span_start,\n",
        "                                    prev_span_end)\n",
        "    if span is not None:\n",
        "        spans.append(span)\n",
        "    return spans\n",
        "\n",
        "\n",
        "# Helper method for si_predictions_to_spans\n",
        "def update_predicted_span(article, label, span_start, span_end, prev_article,\n",
        "                          prev_label, prev_span_start, prev_span_end):\n",
        "    span = None\n",
        "    cur_span_start = prev_span_start\n",
        "    # Ending a span: I-O, B-O, I-B, B-B, new article\n",
        "    if prev_label != 'O' and (label != 'I' or prev_article != article):\n",
        "        span = (prev_article, prev_span_start, prev_span_end)\n",
        "\n",
        "    # Starting a new span: O-B, O-I, I-B, B-B, new article\n",
        "    if label == 'B' or (label == 'I' and prev_label == 'O') \\\n",
        "            or prev_article != article:\n",
        "        # Update the start of the current label span\n",
        "        cur_span_start = span_start\n",
        "    return span, cur_span_start\n",
        "\n",
        "\n",
        "def print_spans(spans, file_prefix, file_stem, file_suffix):\n",
        "    outfile = file_prefix + 'spans_' + file_stem + '_' + file_suffix + '.txt'\n",
        "    with open(outfile, mode='w') as f:\n",
        "        for span in spans:\n",
        "            f.write(str(span[0]) + '\\t' + str(span[1]) + '\\t' +\n",
        "                    str(span[2]) + '\\n')\n",
        "\n",
        "\n",
        "def predict_si(config, model, history, dev_df, dev_raw, dev_x, comments,\n",
        "               file_prefix, file_stem, file_suffix, features,\n",
        "               predict_spans=True):\n",
        "    y_hat = get_bio_predictions(model, dev_x, dev_raw, config.N_CLASSES)\n",
        "    result_df = pd.concat([dev_df, pd.DataFrame(y_hat, columns=['label_pred'])],\n",
        "                          axis=1, sort=False)\n",
        "\n",
        "    logfile = file_prefix + 'log_' + file_stem + '_' + file_suffix + '.txt'\n",
        "\n",
        "    with open(logfile, mode='w') as f:\n",
        "        f.write('DATA PREPROCESSING\\n\\n')\n",
        "        for comment in comments:\n",
        "            comment = comment.replace('#', '')\n",
        "            fields = comment.split(',')\n",
        "            for field in fields:\n",
        "                f.write(comment.strip() + '\\n')\n",
        "        f.write('Additional features:' + str(features) + '\\n')\n",
        "        f.write('\\n\\nCONFIG\\n\\n')\n",
        "        f.write(config.pretty_str())\n",
        "        f.write('\\n\\nMODEL HISTORY\\n\\n')\n",
        "        f.write('Loss ' + config.LOSS + '\\n')\n",
        "        f.write(str(history.history['loss']) + '\\n')\n",
        "        f.write(config.METRIC + '\\n')\n",
        "        f.write(str(history.history[config.METRIC]) + '\\n')\n",
        "        f.write('\\n\\nMODEL SUMMARY\\n\\n')\n",
        "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "    if predict_spans:\n",
        "        spans = si_predictions_to_spans(result_df)\n",
        "        print_spans(spans, file_prefix, file_stem, file_suffix)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "def predict_tc(config, model, history, dev_df, dev_x, comments, file_prefix,\n",
        "               file_stem, file_suffix, features, y_encoder=None):\n",
        "    logfile = file_prefix + 'log_' + file_stem + '_' + file_suffix + '.txt'\n",
        "    with open(logfile, mode='w') as f:\n",
        "        f.write('DATA PREPROCESSING\\n\\n')\n",
        "        for comment in comments:\n",
        "            comment = comment.replace('#', '')\n",
        "            fields = comment.split(',')\n",
        "            for field in fields:\n",
        "                f.write(comment.strip() + '\\n')\n",
        "        f.write('Additional features:' + str(features) + '\\n')\n",
        "        f.write('\\n\\nCONFIG\\n\\n')\n",
        "        f.write(config.pretty_str())\n",
        "        if history:\n",
        "            f.write('\\n\\nMODEL HISTORY\\n\\n')\n",
        "            f.write('Loss ' + config.LOSS + '\\n')\n",
        "            f.write(str(history.history['loss']) + '\\n')\n",
        "            f.write(config.METRIC + '\\n')\n",
        "            f.write(str(history.history[config.METRIC]) + '\\n')\n",
        "            f.write('\\n\\nMODEL SUMMARY\\n\\n')\n",
        "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "    y_hat = model.predict(dev_x)\n",
        "    if y_encoder:\n",
        "        y_hat = y_encoder.inverse_transform(y_hat)\n",
        "    return print_tc(y_hat, dev_df, file_prefix, file_stem, file_suffix)\n",
        "\n",
        "\n",
        "def print_tc(y_hat, dev_df, file_prefix, file_stem, file_suffix):\n",
        "    outfile = file_prefix + 'labels_' + file_stem + '_' + file_suffix + '.txt'\n",
        "    result_df = pd.concat([dev_df, pd.DataFrame(y_hat, columns=['label_pred'])],\n",
        "                          axis=1, sort=False)\n",
        "    result_df = result_df[['document_id', 'label_pred', 'span_start',\n",
        "                           'span_end']]\n",
        "    result_df.to_csv(outfile, sep='\\t', index=False, header=False)\n",
        "    return result_df\n",
        "\n",
        "\n",
        "\n",
        "###########################\n",
        "# Putting it all together #\n",
        "###########################\n",
        "\n",
        "\n",
        "def run(config, file_stem, file_suffix, verbose=True, predict_spans=True,\n",
        "        data=None, word2embedding=None, file_prefix=''):\n",
        "    if verbose:\n",
        "        print('Running with config:')\n",
        "        print(config.pretty_str())\n",
        "    if not data:\n",
        "        if config.LOAD_DATA:\n",
        "            print('Loading data from files')\n",
        "            data = Data(path=config.DATA_PATH)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print('Encoding the data')\n",
        "            data = get_data(config, word2embedding)\n",
        "            if config.SAVE_DATA:\n",
        "                data.save()\n",
        "\n",
        "    if verbose:\n",
        "        print('Additional features:', data.features)\n",
        "        print('Building the model')\n",
        "    if config.TOKEN_LVL:\n",
        "        model, history = get_bilstm(config, data.train_x, data.train_y,\n",
        "                                    data.sample_weight)\n",
        "    else:\n",
        "        history = None\n",
        "        y_encoder = None\n",
        "        if config.MODEL == 'SVM':\n",
        "            model = get_svm(data.train_x, data.train_y)\n",
        "        elif config.MODEL.startswith('FFNN'):\n",
        "            model, history, y_encoder = get_ffnn(config, data.train_x,\n",
        "                                                 data.train_y,\n",
        "                                                 data.sample_weight,\n",
        "                                                 single_layer=config.MODEL == 'FFNN-single')\n",
        "        elif config.MODEL == 'LSTM':\n",
        "            model, history = get_bilstm(config, data.train_x, data.train_y,\n",
        "                                    data.sample_weight)\n",
        "\n",
        "    if verbose:\n",
        "        print('Predicting the test data labels/spans')\n",
        "    if config.TOKEN_LVL:\n",
        "        labels = predict_si(config, model, history, data.dev_df, data.dev_raw,\n",
        "                            data.dev_x, data.comments, file_prefix, file_stem,\n",
        "                            file_suffix, data.features, predict_spans)\n",
        "    else:\n",
        "        labels = predict_tc(config, model, history, data.dev_df, data.dev_x,\n",
        "                            data.comments, file_prefix, file_stem, file_suffix,\n",
        "                            data.features, y_encoder)\n",
        "    if verbose:\n",
        "        print('Done!\\n\\n')\n",
        "\n",
        "    return data, labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EnkETqKZbqT",
        "colab_type": "text"
      },
      "source": [
        "# grid_search.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAjKAZNfSaWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from model import run, si_predictions_to_spans, print_spans\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, args=None):\n",
        "        \"\"\"Creates a default configuration.\n",
        "\n",
        "        Keyword arguments:\n",
        "        args -- a dict(str -> ?) containing values diverging from the default\n",
        "        \"\"\"\n",
        "        # Encoding the data:\n",
        "        self.TOKEN_LVL = True  # True if task 1, False if task 2.\n",
        "        if args and 'TOKEN_LVL' in args:\n",
        "            self.TOKEN_LVL = args['TOKEN_LVL']\n",
        "\n",
        "        self.ONLINE_SOURCES = True  # Input is given via URLs, not local files.\n",
        "\n",
        "        self.UNCASED = True  # If true, words are turned into lower case.\n",
        "        self.FEATURES = None  # If None, the features are determined from the\n",
        "                              # input file.\n",
        "        self.EXCLUDE_FEATURES = []  # Only used if FEATURES is not None\n",
        "        self.SAVE_DATA = False  # If true, the following two values can be used\n",
        "                                # for re-using the data next time.\n",
        "        # In case the training & dev data were saved and can be reused:\n",
        "        self.DATA_PATH = 'gdrive/My Drive/colab_projects/data/data/'\n",
        "        self.LOAD_DATA = False\n",
        "\n",
        "        # Building the model:\n",
        "        self.BATCH_SIZE = 128\n",
        "        self.LSTM_UNITS = 512\n",
        "        self.DROPOUT = 0.25\n",
        "        self.OPTIMIZER = 'adam'\n",
        "        self.METRIC = 'categorical_accuracy'\n",
        "        self.LOSS = 'categorical_crossentropy'\n",
        "\n",
        "        # Making predictions:\n",
        "        self.MAJORITY_VOTING = True\n",
        "\n",
        "        # Task-specific options\n",
        "        if self.TOKEN_LVL:\n",
        "            # Task 1: Span identification\n",
        "            # For using train+dev and test, see the end of this file.\n",
        "            self.N_CLASSES = 2\n",
        "            self.MAX_SEQ_LEN = 35\n",
        "            self.EMBED_DIM = 300\n",
        "            self.EPOCHS = 10\n",
        "            self.CLASS_WEIGHTS = {'O': 1.0, 'I': 6.5, 'B': 6.5}\n",
        "            self.TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-train.tsv?token=AD7GEDPCTVL5SN46K6LG6EC6LP4BW'\n",
        "            self.DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-dev.tsv?token=AD7GEDLCUDJD7YBIGBJVR526LP4BM'\n",
        "            self.EMBEDDING_PATH = 'gdrive/My Drive/colab_projects/data/glove.42B.300d.zip'  # 'gdrive/My Drive/colab_projects/data/glove.6B.100d.zip'\n",
        "            self.USE_BERT = False\n",
        "            self.TRAIN_BERT = 'gdrive/My Drive/colab_projects/data/train_bert-base-uncased.tsv'\n",
        "            self.DEV_BERT = 'gdrive/My Drive/colab_projects/data/dev_bert-base-uncased.tsv'\n",
        "        else:\n",
        "            # Task 2: Technique classification\n",
        "            self.MODEL = 'FFNN'  # Options: 'SVM', 'FFNN', 'FFNN-single', 'LSTM'\n",
        "            self.HIDDEN = 128  # If MODEL == 'FFNN'\n",
        "            self.EPOCHS = 15  # If MODEL == 'FFNN'\n",
        "            self.CLASS_WEIGHTS = None\n",
        "            self.TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-train.tsv?token=AD7GEDOJ3TVR5J4ALHKARP26NO6LO'\n",
        "            self.DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/tc-dev.tsv?token=AD7GEDL5VBEWFJB4IPZ4TXS6NO6LQ'\n",
        "            self.USE_BERT = True  # Currently, we don't have an alternative to this.\n",
        "            self.EMBED_DIM = 768\n",
        "            self.N_BERT_LAYERS = 1\n",
        "            self.TRAIN_BERT = 'gdrive/My Drive/colab_projects/data/tc_train_bert-base-uncased.tsv'\n",
        "            self.DEV_BERT = 'gdrive/My Drive/colab_projects/data/tc_dev_bert-base-uncased.tsv'\n",
        "            self.N_CLASSES = 14\n",
        "            self.MAX_SEQ_LEN = -1  # Value is irrelevant (fixed-size input)\n",
        "\n",
        "        self.FLATTEN = (not self.TOKEN_LVL) or (self.MODEL != 'LSTM')\n",
        "\n",
        "        if args:\n",
        "            for key in args:\n",
        "                setattr(self, key, args[key])                \n",
        "\n",
        "    def pretty_str(self):\n",
        "        return 'max seq len: ' + str(self.MAX_SEQ_LEN) + '\\n' + \\\n",
        "               'embedding depth: ' + str(self.EMBED_DIM) + '\\n' + \\\n",
        "               'BERT embeddings: ' + str(self.USE_BERT) + '\\n' + \\\n",
        "               'BERT layers: ' + str(self.N_BERT_LAYERS) + '\\n' + \\\n",
        "               'TRAIN_BERT: ' + str(self.TRAIN_BERT) + '\\n' + \\\n",
        "               'DEV_BERT: ' + str(self.DEV_BERT) + '\\n' + \\\n",
        "               'number of labels: ' + str(config.N_CLASSES) + '\\n' + \\\n",
        "               'batch size: ' + str(self.BATCH_SIZE) + '\\n' + \\\n",
        "               'epochs: ' + str(self.EPOCHS) + '\\n' + \\\n",
        "               'class weights: ' + str(self.CLASS_WEIGHTS) + '\\n' + \\\n",
        "               'hidden units: ' + str(self.LSTM_UNITS) + '\\n' + \\\n",
        "               'dropout rate: ' + str(self.DROPOUT) + '\\n' + \\\n",
        "               'optimizer: ' + self.OPTIMIZER + '\\n' + \\\n",
        "               'metric: ' + self.METRIC + '\\n' + \\\n",
        "               'loss: ' + self.LOSS + '\\n'\n",
        "\n",
        "\n",
        "def get_majority_vote(votes):\n",
        "    votes = [k for k, _ in sorted(dict(Counter(votes)).items(),\n",
        "                                  key=lambda item: item[1],\n",
        "                                  reverse=True)]\n",
        "    # Task 1: For our data, preferring specific labels in tie situations\n",
        "    # doesn't make a difference.\n",
        "    return votes[0]\n",
        "\n",
        "\n",
        "def run_config(config, file_prefix, data=None, repetitions=5, verbose=True):\n",
        "    now = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
        "    predictions = None\n",
        "    label_cols = []\n",
        "    for i in range(repetitions):\n",
        "        if verbose:\n",
        "            print(\"Iteration \" + str(i + 1) + \" of \" + str(repetitions))\n",
        "        data, labels = run(config, data=data, verbose=verbose,\n",
        "                           file_prefix=file_prefix, file_stem=now,\n",
        "                           file_suffix=str(i))\n",
        "        if config.MAJORITY_VOTING:\n",
        "            if predictions is None:\n",
        "                predictions = labels\n",
        "                predictions = predictions.rename(\n",
        "                    columns={'label_pred': 'label_0'})\n",
        "            else:\n",
        "                predictions.insert(loc=len(predictions.columns),\n",
        "                                   column='label_' + str(i),\n",
        "                                   value=labels.label_pred)\n",
        "            label_cols.append('label_' + str(i))\n",
        "    if config.MAJORITY_VOTING:\n",
        "        labels = []\n",
        "        for row in predictions.itertuples():\n",
        "            labels.append(get_majority_vote(\n",
        "                [getattr(row, l) for l in label_cols]))\n",
        "        predictions['label_pred'] = labels\n",
        "        if config.TOKEN_LVL:\n",
        "            spans = si_predictions_to_spans(predictions)\n",
        "            print_spans(spans, file_prefix, now, 'majority')\n",
        "        else:\n",
        "            print_tc(labels, data.dev_df, file_prefix, now, 'majority')\n",
        "\n",
        "    # Return data in case the next config only changes model features\n",
        "    return data, now\n",
        "\n",
        "\n",
        "file_prefix = '/content/gdrive/My Drive/colab_projects/semeval-predictions/'\n",
        "data = None\n",
        "\n",
        "### Hyperparameter tuning:\n",
        "# for epochs in [5, 15, 20, 25]:\n",
        "#     config = Config({'USE_BERT': True, 'TOKEN_LVL': False, 'EPOCHS': epochs})\n",
        "#     data, _ = run_config(config, file_prefix, data)\n",
        "\n",
        "### You can change config values by passing a dictionary to the constructor:\n",
        "# config = Config({'LOAD_DATA': True})\n",
        "# config = Config({'USE_BERT': True})\n",
        "# config = Config({'USE_BERT': True, 'TOKEN_LVL': False})\n",
        "config = Config({'USE_BERT': True, 'TOKEN_LVL': False, \n",
        "                #  'FEATURES': [],\n",
        "                 'FEATURES': ['repetitions', 'length', 'question'],\n",
        "                 'MODEL': 'FFNN',\n",
        "                #  'CLASS_WEIGHTS': {'Loaded_Language': 1,\n",
        "                #                    'Name_Calling,Labeling': 1,\n",
        "                #                    'Repetition': 2,\n",
        "                #                    'Doubt': 2,\n",
        "                #                    'Exaggeration,Minimisation': 2,\n",
        "                #                    'Appeal_to_fear-prejudice': 2,\n",
        "                #                    'Flag-Waving': 1,\n",
        "                #                    'Causal_Oversimplification': 1,\n",
        "                #                    'Appeal_to_Authority': 1,\n",
        "                #                    'Slogans': 1,\n",
        "                #                    'Black-and-White_Fallacy': 1,\n",
        "                #                    'Whataboutism,Straw_Men,Red_Herring': 1,\n",
        "                #                    'Thought-terminating_Cliches': 1,\n",
        "                #                    'Bandwagon,Reductio_ad_hitlerum': 1}\n",
        "                 'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_bert-base-uncased_10.tsv',\n",
        "                 'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_bert-base-uncased_10.tsv',\n",
        "                 'EMBED_DIM': 768 * 11\n",
        "                #  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/tc_train_bert-large-uncased.tsv',\n",
        "                #  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/tc_dev_bert-large-uncased.tsv',\n",
        "                #  'EMBED_DIM': 1024,\n",
        "                #  'TRAIN_BERT': '/content/gdrive/My Drive/colab_projects/data/full_bert_train.tsv',\n",
        "                #  'DEV_BERT': '/content/gdrive/My Drive/colab_projects/data/full_bert_dev.tsv',\n",
        "                #  'EMBED_DIM': 14 + 768,\n",
        "                #  'UNCASED': False\n",
        "                 })\n",
        "data, now = run_config(config, file_prefix, data)\n",
        "\n",
        "### For predictions on the final test set (task 1):\n",
        "# config = Config({'USE_BERT': True,\n",
        "#                  'TRAIN_URL': 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-train%2Bdev.tsv?token=AD7GEDJ7GSTS3RSP5ZSXLZ26LP4BS',\n",
        "#                  'DEV_URL': 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-test.tsv?token=AD7GEDM7A3GFIAEZHHESFO26LP4BQ',\n",
        "#                  'TRAIN_BERT': 'gdrive/My Drive/colab_projects/data/train+dev_bert-base-uncased.tsv',\n",
        "#                  'DEV_BERT': 'gdrive/My Drive/colab_projects/data/test_bert-base-uncased.tsv'\n",
        "#                  })\n",
        "# data, now = run_config(config, file_prefix, data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtkx52hD-mVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now = '20200307-151259'\n",
        "\n",
        "for sfx in ['0', '1', '2', '3', '4', 'majority']:\n",
        "    f = file_prefix + 'labels_' + now + '_' + sfx + '.txt'\n",
        "    df = pd.read_csv(f, sep='\\t', usecols=[1], names=['label'])\n",
        "    df = df['label'].value_counts().rename_axis('labels').reset_index(name='counts')\n",
        "    df['%'] = df['counts'] / df['counts'].sum()\n",
        "    print('labels_' + now + '_' + sfx)\n",
        "    print(df)\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}