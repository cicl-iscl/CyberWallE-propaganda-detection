{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_baseline_TC.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlPBMsP2nX8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VkraAilfUNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdpigBNWorJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DWPQ8clozwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGC8wJFo6ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn3lryPUepi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbc_aTh8pZbY",
        "colab_type": "text"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHHTk_1xcGRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/f52d4a981c5bd45436951f4474759b684ff59fa7/data/dumb-train-task2-TC-with-spans.txt?token=AD7GEDPEQYQYNPM2FMX7AH26NNPCE'\n",
        "DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/f52d4a981c5bd45436951f4474759b684ff59fa7/data/dev-task2-TC-with-spans-with-repetition.txt?token=AD7GEDJAL6XK2C6SXLIDCYK6NNPBG'\n",
        "\n",
        "SAVE_LAYER_REP = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq323hsYfM2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(TRAIN_URL, sep='\\t', quoting=3, header=None, names=[\"document_id\", \"label\", 'span_start', 'span_end', \"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNJc4mUTuzX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "LE = LabelEncoder()\n",
        "train_df['label_int'] = LE.fit_transform(train_df['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ySET4Zfbi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z25_ZD9Xfdcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqAv0cQIx5LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max(train_df[\"label_int\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kKzvY5wg_ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = train_df.text.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = train_df.label_int.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS13GPBZqb5w",
        "colab_type": "text"
      },
      "source": [
        "## Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqanEztXqWQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjMLSr1lr2kt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s8mdfCzr5r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWzrVdQzr9-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkDdPDRgsBCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfkTqjvLckuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# span_infos = []\n",
        "# for row in train_df.itertuples():\n",
        "#     span_infos.append((row.document_id, row.span_start, row.span_end, row.text))\n",
        "# span_infos = torch.tensor(span_infos)\n",
        "spans = train_df.text.tolist()\n",
        "span_ids = list(range(len(spans)))\n",
        "span_ids = torch.tensor(span_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sVNFBuCs4hc",
        "colab_type": "text"
      },
      "source": [
        "Create the attention masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HiZpLeGs49H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2phNQr__s9Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "# train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "#                                                             random_state=2018, test_size=0.1)\n",
        "# train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "#                                              random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWuScacaB__1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = input_ids\n",
        "train_labels = labels\n",
        "train_masks = attention_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWbfkRs6tFqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "# validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "# validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "# validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uadyi_GfxIBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels, span_ids)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "# validation_sampler = SequentialSampler(validation_data)\n",
        "# validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63lxc2GKxTjk",
        "colab_type": "text"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7joGviRRxW4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=14)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEXb9Z3fyz26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVt97QAcy91h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odUm3uOQzOmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bNObYYXzSRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SAVE_LAYER_REP:\n",
        "    ROUNDING_ACC = 9\n",
        "    OUT_FILE = '/content/gdrive/My Drive/colab_projects/data/bertforseq-train.tsv'\n",
        "    f = open(OUT_FILE, 'w', encoding='utf-8')\n",
        "\n",
        "t = [] \n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "n_epochs = 4\n",
        "\n",
        "\n",
        "# for epoch in trange(1, n_epochs + 1, desc=\"Epoch\"):\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    print('Epoch', epoch)\n",
        "\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    \n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels, b_span_ids = batch\n",
        "        # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        train_loss_set.append(loss.item())    \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1 \n",
        "\n",
        "        if SAVE_LAYER_REP and epoch == n_epochs:\n",
        "            # Save predictions (pre-softmax)\n",
        "            layers = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            b_span_ids = b_span_ids.tolist()\n",
        "            for entry in range(layers.size(0)):\n",
        "                predictions = layers[entry].detach().cpu().numpy()\n",
        "                values = [round(x, ROUNDING_ACC) for x in predictions]\n",
        "                # for info in b_span_infos[entry].tolist():\n",
        "                #     f.write(str(info) + '\\t')\n",
        "                f.write(str(step) + '\\t' + 'class' + '\\t')\n",
        "                f.write(spans[b_span_ids[entry]] + '\\t')\n",
        "                f.write(str(values) + '\\n')\n",
        "\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "if SAVE_LAYER_REP:\n",
        "    f.close()\n",
        "    \n",
        "  # # Validation\n",
        "\n",
        "  # # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  # model.eval()\n",
        "\n",
        "  # # Tracking variables \n",
        "  # eval_loss, eval_accuracy = 0, 0\n",
        "  # nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # # Evaluate data for one epoch\n",
        "  # for batch in validation_dataloader:\n",
        "  #   # Add batch to GPU\n",
        "  #   batch = tuple(t.to(device) for t in batch)\n",
        "  #   # Unpack the inputs from our dataloader\n",
        "  #   b_input_ids, b_input_mask, b_labels = batch\n",
        "  #   # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "  #   with torch.no_grad():\n",
        "  #     # Forward pass, calculate logit predictions\n",
        "  #     logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "  #   # Move logits and labels to CPU\n",
        "  #   logits = logits.detach().cpu().numpy()\n",
        "  #   label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  #   tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "  #   eval_accuracy += tmp_eval_accuracy\n",
        "  #   nb_eval_steps += 1\n",
        "\n",
        "  # print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN3caG8h1n6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.figure(figsize=(15,8))\n",
        "# plt.title(\"Training loss\")\n",
        "# plt.xlabel(\"Batch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.plot(train_loss_set)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk3gnMXy1yyI",
        "colab_type": "text"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VffNUsN10EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(DEV_URL, sep='\\t', quoting=3, header=None, usecols=[0, 2, 3, 4], names=[\"document_id\", 'span_start', 'span_end', \"text\", ])\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvmSIYUG2RIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "test_sentences = test_df.text.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "test_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in test_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwkyMHpp2uy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wZ_CfKb2_V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "\n",
        "spans = test_df.text.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJQtC7DN4MB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Ove3tu4T1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "preds = []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  \n",
        "  # Store predictions and\n",
        "  preds.append(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL_yElpo4nh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = [item for sublist in preds for item in sublist]\n",
        "flat_predictions = np.argmax(predictions, axis=1).flatten()\n",
        "flat_predictions[:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzG5EN1846xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SAVE_LAYER_REP:\n",
        "    ROUNDING_ACC = 9\n",
        "    OUT_FILE = '/content/gdrive/My Drive/colab_projects/data/bertforseq-dev.tsv'\n",
        "    with open(OUT_FILE, 'w', encoding='utf-8') as f:\n",
        "        for pred, span in zip(predictions, spans):\n",
        "            f.write('1\\tclass\\t' + span + '\\t')\n",
        "            values = [round(x, ROUNDING_ACC) for x in pred]\n",
        "            f.write(str(values) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNrZnAOQ5vf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv(DEV_URL, sep='\\t', quoting=3, header=None, names=[\"document_id\", \"label\", \"from_idx\", \"to_idx\", \"text\", \"rep\"])\n",
        "predicted_labels = LE.inverse_transform(flat_predictions)\n",
        "test_df[\"label\"] = predicted_labels\n",
        "test_df.head(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGLnW2pP6-qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del test_df[\"text\"]\n",
        "del test_df[\"rep\"]\n",
        "test_df.to_csv(\"dev-task-TC.txt\", sep='\\t', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}