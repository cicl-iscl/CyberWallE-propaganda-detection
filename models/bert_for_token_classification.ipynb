{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert for token classication.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjOkb7ZH161F",
        "colab_type": "text"
      },
      "source": [
        "First draft based on https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/\n",
        "\n",
        "Needs to be cleaned up and improved :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRZKVwilvnSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6wZS-Vz2VXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHJxYR06vgrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import urllib\n",
        "import pandas as pd\n",
        "from pytorch_pretrained_bert import BertForTokenClassification, BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.optim import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZnTBzZvymH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 50\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43x0CqLkyprJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULEtkihxve5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjr-RXIz8j0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/train-improved-sentiwordnet-arguingfullindiv-pos.tsv?token=AD7GEDK3MI27HVJPQWOE74C6FBZHA'\n",
        "DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/dev-improved-sentiwordnet-arguingfullindiv-pos.tsv?token=AD7GEDM3LOMZM6MP4HZS4MS6FBZHK'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw53eIej-9wM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_comments(filename, url=True):\n",
        "    if url:\n",
        "        comments = []\n",
        "        with urllib.request.urlopen(filename) as f:\n",
        "            for line in f:\n",
        "                if line.startswith(b'#'):\n",
        "                    comments.append(line.decode(\"utf-8\"))\n",
        "                else:\n",
        "                    break\n",
        "        return comments\n",
        "    with open(filename, 'r', encoding='utf8') as f:\n",
        "        commentiter = takewhile(lambda s: s.startswith('#'), f)\n",
        "        comments = list(commentiter)\n",
        "    return comments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLM64d11-YOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comments = get_comments(TRAIN_URL)\n",
        "train_df = pd.read_csv(TRAIN_URL, sep='\\t', skiprows=len(comments), quoting=3)\n",
        "train_input = train_df.groupby('sent_id')['token'].apply(list).to_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGq7Le6PDuig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = train_input['token'].tolist()\n",
        "sentences = [['[CLS]'] + [str(word).lower() for word in sent] + ['[SEP]'] for sent in sentences]\n",
        "sentences[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uogGVFboFVLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = train_df.groupby('sent_id')['label'].apply(list).to_frame()['label'].tolist()\n",
        "labels[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PeFKT4DIF2co",
        "colab": {}
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(' '.join(sent)) for sent in sentences]\n",
        "print(tokenized_texts[0])\n",
        "print(tokenized_texts[8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kf5MqN2GKUL9",
        "colab": {}
      },
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "tag2idx = {'O': 0, 'B': 1, 'I': 1}\n",
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7nW5h1tNMOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOMc9x82z_zS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdC3ZUGr55pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WejUWUve6P13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags_vals = ['O', 'I']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWFCF1Ep5UnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import trange\n",
        "epochs = 3\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # TRAIN loop\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None,\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    # VALIDATION on validation set\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                                  attention_mask=b_input_mask, labels=b_labels)\n",
        "            logits = model(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.append(label_ids)\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        \n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "    eval_loss = eval_loss/nb_eval_steps\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
        "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
        "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "    print(pred_tags)\n",
        "    print(valid_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL66tsckMoJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comments = get_comments(DEV_URL)\n",
        "dev_df = pd.read_csv(DEV_URL, sep='\\t', skiprows=len(comments), quoting=3)\n",
        "dev_input = dev_df.groupby('sent_id')['token'].apply(list).to_frame()\n",
        "sentences = dev_input['token'].tolist()\n",
        "sentences = [['[CLS]'] + [str(word).lower() for word in sent] + ['[SEP]'] for sent in sentences]\n",
        "sentences[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lSHfrFhM7hO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(' '.join(sent)) for sent in sentences]\n",
        "print(tokenized_texts[0])\n",
        "print(tokenized_texts[8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txsJMJBsNBWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLemOpxuNFO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_inputs = torch.tensor(input_ids)\n",
        "pred_masks = torch.tensor(attention_masks)\n",
        "\n",
        "pred_data = TensorDataset(pred_inputs, pred_masks)\n",
        "pred_sampler = SequentialSampler(pred_data)\n",
        "pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_9Eh0rJNfW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUggEHrWNikq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "\n",
        "# Predict \n",
        "for batch in pred_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "#   predictions.append(logits)\n",
        "  predictions.extend([list(p) for p in np.argmax(logits, axis=2)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2m6cXL0Ts2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = []\n",
        "idx2label = {0: 'O', 1: 'I'}\n",
        "for mask, pred in zip(attention_masks, predictions):\n",
        "    sent_pred = []\n",
        "    for m, p in zip(mask, pred):\n",
        "        if m > 0.0:\n",
        "            sent_pred.append(idx2label[p])\n",
        "    preds.append(sent_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecugg-PfUWmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_input['n_toks'] = dev_input['token'].apply(lambda x: len(x))\n",
        "dev_input.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cu5nJQVXd_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(tokenized_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNbWebKXWRW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_with_toolong = []\n",
        "for row in dev_input.itertuples():\n",
        "    sent_idx = row.Index - 1\n",
        "    sent_pred = preds[sent_idx]\n",
        "    n_actual_toks = row.n_toks\n",
        "    n_pred_toks = len(sent_pred)\n",
        "\n",
        "    pred_without_subwordtoks = []\n",
        "    for tok, pred in zip(tokenized_texts[sent_idx], sent_pred):\n",
        "        if tok == '[CLS]' or tok == '[SEP]':\n",
        "            continue\n",
        "        if tok.startswith('##'):\n",
        "            pred_without_subwordtoks[-1] = pred\n",
        "        else:\n",
        "            pred_without_subwordtoks.append(pred)\n",
        "\n",
        "    for _ in range(n_actual_toks - len(pred_without_subwordtoks)):\n",
        "        pred_without_subwordtoks.append('O')\n",
        "    preds_with_toolong.append(pred_without_subwordtoks)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R-03JZYNlOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(preds_with_toolong)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjhSttfNXEVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flat_predictions = [item for sublist in preds_with_toolong for item in sublist]\n",
        "len(flat_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eahcYhzDXHuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsD6x_poNrVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_df[\"label\"] = flat_predictions\n",
        "dev_df.head()\n",
        "dev_df.to_csv(\"dev-task-SI.out\", sep='\\t', header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT_TZYCjZCKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def si_predictions_to_spans(label_df):\n",
        "    spans = []\n",
        "    prev_label = 'O'\n",
        "    prev_span_start = '-1'\n",
        "    prev_span_end = '-1'\n",
        "    prev_article = ''\n",
        "\n",
        "    for row in label_df.itertuples():\n",
        "        article = row.document_id\n",
        "        span_start = row.token_start\n",
        "        span_end = row.token_end\n",
        "        label = row.label\n",
        "\n",
        "        span, prev_span_start = update_prediction(article, label,\n",
        "                                                  span_start, span_end,\n",
        "                                                  prev_article, prev_label,\n",
        "                                                  prev_span_start,\n",
        "                                                  prev_span_end)\n",
        "        if span is not None:\n",
        "            spans.append(span)\n",
        "\n",
        "        prev_article = article\n",
        "        prev_label = label\n",
        "        prev_span_end = span_end\n",
        "\n",
        "    # Make sure we get the last prediction\n",
        "    span, _ = update_prediction(article, label, span_start, span_end,\n",
        "                                prev_article, prev_label, prev_span_start,\n",
        "                                prev_span_end)\n",
        "    if span is not None:\n",
        "        spans.append(span)\n",
        "    return spans\n",
        "\n",
        "# Helper method for si_predictions_to_spans\n",
        "def update_prediction(article, label, span_start, span_end, prev_article,\n",
        "                      prev_label, prev_span_start, prev_span_end):\n",
        "    span = None\n",
        "    cur_span_start = prev_span_start\n",
        "    # Ending a span: I-O, B-O, I-B, B-B, new article\n",
        "    if prev_label != 'O' and (label != 'I' or prev_article != article):\n",
        "        span = (prev_article, prev_span_start, prev_span_end)\n",
        "\n",
        "    # Starting a new span: O-B, O-I, I-B, B-B, new article\n",
        "    if label == 'B' or (label == 'I' and prev_label == 'O') \\\n",
        "            or prev_article != article:\n",
        "        # Update the start of the current label span\n",
        "        cur_span_start = span_start\n",
        "    return span, cur_span_start\n",
        "\n",
        "\n",
        "def print_spans(spans, file_prefix, file_stem, file_suffix):\n",
        "    outfile = file_prefix + 'spans_' + file_stem + '_' + file_suffix + '.txt'\n",
        "    with open(outfile, mode='w') as f:\n",
        "        for span in spans:\n",
        "            f.write(str(span[0]) + '\\t' + str(span[1]) + '\\t' +\n",
        "                    str(span[2]) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJlgM8HxZHj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spans = si_predictions_to_spans(dev_df)\n",
        "print_spans(spans, '', 'bert', '')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}