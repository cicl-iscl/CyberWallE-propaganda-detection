{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sequence_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlPBMsP2nX8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdpigBNWorJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DWPQ8clozwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGC8wJFo6ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-0Y9_7lLYMn",
        "colab_type": "text"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS9qKJinLZ7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/f52d4a981c5bd45436951f4474759b684ff59fa7/data/dumb-train-task2-TC-with-spans.txt?token=AD7GEDPEQYQYNPM2FMX7AH26NNPCE'\n",
        "TEST_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/f52d4a981c5bd45436951f4474759b684ff59fa7/data/dev-task2-TC-with-spans-with-repetition.txt?token=AD7GEDJAL6XK2C6SXLIDCYK6NNPBG'\n",
        "\n",
        "\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 12\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP = .1\n",
        "N_EPOCHS = 4  # 2-4 recommended\n",
        "BERT_MODEL = 'bert-base-cased'\n",
        "\n",
        "\n",
        "SAVE_LAYER_REP = True\n",
        "ROUNDING_ACC = 9\n",
        "\n",
        "\n",
        "UNCASED = 'uncased' in BERT_MODEL\n",
        "FILE_PREFIX = 'gdrive/My Drive/colab_projects/'\n",
        "now = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
        "PREDICTIONS_FILE = FILE_PREFIX + 'semeval-predictions/labels_bert_' + now + '.txt'\n",
        "LOG_FILE = FILE_PREFIX + 'semeval-predictions/log_bert_' + now + '.txt'\n",
        "BERT_FILE_TRAIN = FILE_PREFIX + 'data/tc_train_' + now + '.tsv'\n",
        "BERT_FILE_TEST= FILE_PREFIX + 'data/tc_test_' + now + '.tsv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbc_aTh8pZbY",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS4_qt4RMbBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(url, training=True):\n",
        "    df = pd.read_csv(url, sep='\\t', quoting=3, header=None,\n",
        "                     usecols=[0, 1, 2, 3, 4],\n",
        "                     names=['document_id', 'label', 'span_start', 'span_end',\n",
        "                            'text'])\n",
        "    labels = None\n",
        "    label_encoder = None\n",
        "    if training:\n",
        "        label_encoder = LabelEncoder()\n",
        "        labels = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in df.text.values]\n",
        "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL,\n",
        "                                              do_lower_case=not UNCASED)\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                              truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    attention_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Used for extracting the data in the right order:\n",
        "    spans = df.text.tolist()\n",
        "    span_ids = list(range(len(spans)))\n",
        "\n",
        "    if training:\n",
        "        data = TensorDataset(torch.tensor(input_ids),\n",
        "                             torch.tensor(attention_masks),\n",
        "                             torch.tensor(labels),\n",
        "                             torch.tensor(span_ids))\n",
        "        sampler = RandomSampler(data)\n",
        "    else:\n",
        "        data = TensorDataset(torch.tensor(input_ids),\n",
        "                             torch.tensor(attention_masks))\n",
        "        sampler = SequentialSampler(data)\n",
        "\n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "    return df, label_encoder, dataloader, spans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uadyi_GfxIBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, label_encoder, train_dataloader, spans_train = get_data(TRAIN_URL)\n",
        "test_df, _, test_dataloader, spans_test = get_data(TEST_URL, training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63lxc2GKxTjk",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7joGviRRxW4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=14)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEXb9Z3fyz26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=LEARNING_RATE,\n",
        "                     warmup=WARMUP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bNObYYXzSRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entries = []\n",
        "train_loss_steps = []\n",
        "train_loss_epochs = []\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch', epoch)\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)  # Add batch to GPU\n",
        "\n",
        "        b_input_ids, b_input_mask, b_labels, b_span_ids = batch\n",
        "\n",
        "        optimizer.zero_grad()  # Clear out the gradients (by default they accumulate)\n",
        "        # Forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None,\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        train_loss_steps.append(loss.item())    \n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update parameters and take a step using the computed gradient\n",
        "        \n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1 \n",
        "\n",
        "        if SAVE_LAYER_REP and epoch == N_EPOCHS:\n",
        "            # Save predictions (pre-softmax)\n",
        "            layers = model(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "            b_span_ids = b_span_ids.tolist()\n",
        "            for entry in range(layers.size(0)):\n",
        "                predictions = layers[entry].detach().cpu().numpy()\n",
        "                values = [round(x, ROUNDING_ACC) for x in predictions]\n",
        "                entries.append((b_span_ids[entry],\n",
        "                                spans_train[b_span_ids[entry]],\n",
        "                                str(values)))\n",
        "\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    train_loss_epochs.append(tr_loss/nb_tr_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7N4mO-pU5mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SAVE_LAYER_REP:\n",
        "    entries = sorted(entries, key=lambda entry: entry[0])\n",
        "    with open(BERT_FILE_TRAIN, 'w', encoding='utf-8') as f:\n",
        "        for entry in entries:\n",
        "            f.write('1\\tclass\\t' + entry[1] + '\\t' + entry[2] + '\\n')\n",
        "\n",
        "with open(LOG_FILE, 'w', encoding='utf-8') as f:\n",
        "    f.write('MAX_LEN: ' + str(MAX_LEN) + '\\n')\n",
        "    f.write('BATCH_SIZE: ' + str(BATCH_SIZE) + '\\n')\n",
        "    f.write('LEARNING_RATE: ' + str(LEARNING_RATE) + '\\n')\n",
        "    f.write('WARMUP: ' + str(WARMUP) + '\\n')\n",
        "    f.write('N_EPOCHS: ' + str(N_EPOCHS) + '\\n')\n",
        "    f.write('BERT_MODEL: ' + BERT_MODEL + '\\n')\n",
        "    f.write('TRAIN LOSS BY EPOCH: ' + str(train_loss_epochs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk3gnMXy1yyI",
        "colab_type": "text"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Ove3tu4T1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "preds = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)  # Add batch to GPU\n",
        "    b_input_ids, b_input_mask = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        logits = model(b_input_ids, token_type_ids=None,\n",
        "                       attention_mask=b_input_mask)\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()  # Move logits and labels to CPU\n",
        "    preds.append(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL_yElpo4nh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = [item for sublist in preds for item in sublist]\n",
        "flat_predictions = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "if SAVE_LAYER_REP:\n",
        "    with open(BERT_FILE_TEST, 'w', encoding='utf-8') as f:\n",
        "        for pred, span in zip(predictions, spans_test):\n",
        "            f.write('1\\tclass\\t' + span + '\\t')\n",
        "            values = [round(x, ROUNDING_ACC) for x in pred]\n",
        "            f.write(str(values) + '\\n')\n",
        "\n",
        "predicted_labels = label_encoder.inverse_transform(flat_predictions)\n",
        "test_df['label'] = predicted_labels\n",
        "del test_df['text']\n",
        "test_df.to_csv(PREDICTIONS_FILE, sep='\\t', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}