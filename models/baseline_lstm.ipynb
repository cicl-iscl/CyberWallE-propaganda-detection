{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline-lstm",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VubK6qZQUmeM",
        "colab_type": "text"
      },
      "source": [
        "# Challenges in NLP, WS19/20\n",
        "\n",
        "Blaschke Verena, ISCL MA<br/>\n",
        "Korniyenko Maxim, ISCL MA<br/>\n",
        "Tureski Sam, ML MA<br/>\n",
        "\n",
        "-----\n",
        "## Baseline model for Span Identification task\n",
        "-----\n",
        "\n",
        "The working process looks like the following:\n",
        "- Data preparation.\n",
        "- Creating the model.\n",
        "- Training the model.\n",
        "- Testing the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddQhWcO8_jIX",
        "colab_type": "code",
        "outputId": "fffa4536-76ff-48aa-e08a-18f0a81b3867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSkNNNrI9ZxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "from enum import Enum\n",
        "from itertools import takewhile\n",
        "import urllib.request\n",
        "import time\n",
        "\n",
        "# Creating the model\n",
        "from keras.layers import Bidirectional, CuDNNLSTM, Dense, Dropout, TimeDistributed\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Results analysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlVCNsD2-rfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# installing tools for oversampling\n",
        "# !pip install -U imbalanced-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuZXRhTYI8y7",
        "colab_type": "text"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tWOkKZNBbxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:\n",
        "  def __init__(self):\n",
        "    self.MAX_SEQ_LEN = 35\n",
        "    self.EMBED_DIM = 100\n",
        "    self.BATCH_SIZE = 32\n",
        "    self.EPOCHS = 10\n",
        "\n",
        "    self.N_CLASSES = 2\n",
        "    self.O_WEIGHT = 1.0\n",
        "    self.I_WEIGHT = 6.5\n",
        "    self.B_WEIGHT = 6.5\n",
        "\n",
        "    self.LSTM_UNITS = 512\n",
        "    self.DROPOUT = 0.25\n",
        "    self.OPTIMIZER = 'adam'\n",
        "    self.METRIC = 'categorical_accuracy'\n",
        "    self.LOSS = 'categorical_crossentropy'\n",
        "\n",
        "    self.TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/train-data-improved-sentiwordnet-arguingfull.tsv?token=AD7GEDLFTVHGUIDOG4EDKYK57FJJY'\n",
        "    self.DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/dev-improved-sentiwordnet-arguingfull.tsv?token=AD7GEDKHMNRQLNNRBNDYWJK57FJJ6'\n",
        "\n",
        "    self.EMBEDDING_PATH = 'gdrive/My Drive/colab_projects/data/glove.6B.100d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7UDJPQ3JLeL",
        "colab_type": "text"
      },
      "source": [
        "# Input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9px5zsuDQt5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Version for files that are not specified via a URL:\n",
        "# def get_comments(filename):\n",
        "#   with open(filename, 'r', encoding='utf8') as f:\n",
        "#     commentiter = takewhile(lambda s: s.startswith('#'), f)\n",
        "#     comments = list(commentiter)\n",
        "#   return comments\n",
        "\n",
        "def get_comments(filename):\n",
        "  comments = []\n",
        "  with urllib.request.urlopen(filename) as f:\n",
        "    for line in f:\n",
        "      if line.startswith(b'#'):\n",
        "        comments.append(line)\n",
        "      else:\n",
        "        break\n",
        "  return comments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTMwJaN0Essg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cols(input_df, col):\n",
        "  return input_df.groupby('sent_id')[col].apply(list).to_frame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC0ApJANkvmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_sent_lens(input_df, col='token'):\n",
        "  input_df['n_toks'] = input_df[col].apply(lambda x: len(x))\n",
        "  return input_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKIY6wegeyKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(input_df, feature_cols):\n",
        "  x = add_sent_lens(get_cols(input_df, 'token'))\n",
        "  for feature in feature_cols:\n",
        "    x = pd.merge(left=x, right=get_cols(input_df, feature),\n",
        "                 left_on='sent_id', right_on='sent_id')\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOIFqkxbHJfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_x(x, word2embedding, feature_header, max_seq_len, embed_dim):\n",
        "  embedding_matrix = np.zeros([len(x),\n",
        "                               max_seq_len,\n",
        "                               embed_dim + len(feature_header)])\n",
        "  for row in x.itertuples():\n",
        "    sent_idx = row.Index - 1\n",
        "    for tok_idx in range(row.n_toks):\n",
        "      word = row.token[tok_idx]\n",
        "      embedding_matrix[sent_idx][tok_idx][:embed_dim] = \\\n",
        "        word2embedding.get(word, np.random.randn(embed_dim))\n",
        "      for i, feature in enumerate(feature_header):\n",
        "        embedding_matrix[sent_idx][tok_idx][embed_dim + i] = \\\n",
        "          getattr(row, feature)[tok_idx]\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee6N07KeGoZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_y(y, label2idx, max_seq_len, n_classes):\n",
        "  if n_classes == 1:\n",
        "    labels = np.zeros([len(y), max_seq_len])\n",
        "  else:\n",
        "    labels = np.zeros([len(y), max_seq_len, n_classes])\n",
        "\n",
        "  for row in y.itertuples():\n",
        "    sent_idx = row.Index - 1\n",
        "    for tok_idx, label in enumerate(row.label):\n",
        "      labels[sent_idx][tok_idx] = label2idx[label]\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ORZhvz69htt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(config, word2embedding, training):\n",
        "  # We're getting the comments this way so we can:\n",
        "  # - add them to the output\n",
        "  # - parse lines that contain '#' as the token\n",
        "  if training:\n",
        "    infile = config.TRAIN_URL\n",
        "  else:\n",
        "    infile = config.DEV_URL\n",
        "  comments = get_comments(infile)\n",
        "  df = pd.read_csv(infile, sep='\\t', skiprows=len(comments), quoting=3)\n",
        "\n",
        "  std_cols = ['document_id', 'sent_id', 'token_start',\n",
        "              'token_end', 'token', 'label']\n",
        "  feature_cols = []\n",
        "  for col in df.columns:\n",
        "    if col not in std_cols:\n",
        "      feature_cols.append(col)\n",
        "\n",
        "  x_raw = get_features(df, feature_cols)\n",
        "  x_enc = encode_x(x_raw, word2embedding, feature_cols,\n",
        "                   config.MAX_SEQ_LEN, config.EMBED_DIM)\n",
        "\n",
        "  y = None\n",
        "  sample_weight = None\n",
        "  if 'label' in df.columns:\n",
        "    y_raw = get_cols(df, 'label')\n",
        "    if config.N_CLASSES == 3:\n",
        "      label2idx = {\"O\": [1, 0, 0], \"B\": [0, 0, 1], \"I\": [0, 1, 0]}\n",
        "    elif config.N_CLASSES == 2:\n",
        "      label2idx = {\"O\": [1, 0], \"B\": [0, 1], \"I\": [0, 1]}\n",
        "    y = encode_y(y_raw, label2idx, config.MAX_SEQ_LEN, config.N_CLASSES)\n",
        "    label2weight = {'O': config.O_WEIGHT, 'I': config.I_WEIGHT,\n",
        "                    'B': config.B_WEIGHT}\n",
        "    sample_weight = encode_y(y_raw, label2weight, config.MAX_SEQ_LEN,\n",
        "                             n_classes=1)\n",
        "  \n",
        "  return df, x_raw, x_enc, sample_weight, y, comments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d2WUqEnWMPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = Config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ab1jvz0H6hV",
        "colab_type": "code",
        "outputId": "74e05598-5501-4bd1-d446-5f490e5a0162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word2embedding = {}\n",
        "f = open(config.EMBEDDING_PATH)\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word2embedding[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(word2embedding))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzmv6p8heoJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, _, train_x, sample_weight, train_y, comments = prepare_data(config,\n",
        "                                                               word2embedding,\n",
        "                                                               training=True)\n",
        "dev_df, dev_raw, dev_x, _, _, _ = prepare_data(config, word2embedding,\n",
        "                                               training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mPJx2gI0f5j",
        "colab_type": "code",
        "outputId": "545d54d6-d5ea-45dd-e4fc-456fac7f1e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "print(train_x.shape)\n",
        "print(dev_x.shape)\n",
        "print(train_y.shape)\n",
        "print(sample_weight[2])\n",
        "dev_raw.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21501, 35, 103)\n",
            "(3830, 35, 103)\n",
            "(21501, 35, 2)\n",
            "[1.  6.5 6.5 6.5 6.5 6.5 6.5 6.5 6.5 6.5 1.  1.  1.  1.  1.  1.  1.  1.\n",
            " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0. ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>n_toks</th>\n",
              "      <th>positive</th>\n",
              "      <th>negative</th>\n",
              "      <th>arglex</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sent_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Police, had, previously, gone, to, home, wher...</td>\n",
              "      <td>12</td>\n",
              "      <td>[0.0625, 0.01875, 0.0, 0.03125, 0.0, 0.0147058...</td>\n",
              "      <td>[0.0, 0.05, 0.0, 0.40625, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLEVELAND, —, Police, invstigating, domestic,...</td>\n",
              "      <td>31</td>\n",
              "      <td>[0.0, 0.0, 0.0625, 0.0, 0.02083333333333333, 0...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.05, 0.0, 0....</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[police, reports, from, the, Columbus, suburb,...</td>\n",
              "      <td>10</td>\n",
              "      <td>[0.0625, 0.038461538461538464, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.009615384615384616, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Westerville, Officers, Eric, Joering, ,, 39, ...</td>\n",
              "      <td>34</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[The, suspect, ,, 30-year, -, old, Quentin, Sm...</td>\n",
              "      <td>30</td>\n",
              "      <td>[0.0, 0.125, 0.0, 0.0, 0.0, 0.1527777777777778...</td>\n",
              "      <td>[0.0, 0.20833333333333331, 0.0, 0.0, 0.0, 0.06...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     token  ...                                             arglex\n",
              "sent_id                                                     ...                                                   \n",
              "1        [Police, had, previously, gone, to, home, wher...  ...               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "2        [CLEVELAND, —, Police, invstigating, domestic,...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3        [police, reports, from, the, Columbus, suburb,...  ...                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "4        [Westerville, Officers, Eric, Joering, ,, 39, ...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "5        [The, suspect, ,, 30-year, -, old, Quentin, Sm...  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HboCSN-CJPrH",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gxxeBUClDOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def custom_loss(y_true, y_pred):\n",
        "#   # for test purposes\n",
        "#   return K.variable(value=np.ones(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuzEvEcGI1xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bilstm(input_shape, config):\n",
        "  model = Sequential()\n",
        "  model.add(Bidirectional(CuDNNLSTM(config.LSTM_UNITS, return_sequences=True),\n",
        "                          input_shape=input_shape))\n",
        "  model.add(Dropout(config.DROPOUT))\n",
        "  model.add(TimeDistributed(Dense(config.N_CLASSES, activation='softmax')))\n",
        "  model.compile(\n",
        "                # loss = custom_loss,\n",
        "                loss=config.LOSS,\n",
        "                optimizer=config.OPTIMIZER,\n",
        "                metrics=[config.METRIC],\n",
        "                sample_weight_mode='temporal')\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwW0XSaMJwZN",
        "colab_type": "code",
        "outputId": "a9038c3a-e769-454c-80de-e74029d93961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "model = get_bilstm(train_x.shape[1:], config)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_4 (Bidirection (None, 35, 1024)          2527232   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 35, 1024)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 35, 2)             2050      \n",
            "=================================================================\n",
            "Total params: 2,529,282\n",
            "Trainable params: 2,529,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXQH38939TCJ",
        "colab_type": "code",
        "outputId": "45c6fbaa-451c-4fb1-f8e5-2040dea3ecaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "source": [
        "history = model.fit(train_x, train_y,\n",
        "                    epochs=config.EPOCHS,\n",
        "                    batch_size=config.BATCH_SIZE,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1,\n",
        "                    sample_weight=sample_weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 19350 samples, validate on 2151 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "19350/19350 [==============================] - 17s 899us/step - loss: 1.0698 - categorical_accuracy: 0.7439 - val_loss: 1.2360 - val_categorical_accuracy: 0.7694\n",
            "Epoch 2/10\n",
            "19350/19350 [==============================] - 10s 516us/step - loss: 1.0210 - categorical_accuracy: 0.7444 - val_loss: 1.2166 - val_categorical_accuracy: 0.7582\n",
            "Epoch 3/10\n",
            "19350/19350 [==============================] - 10s 520us/step - loss: 0.9794 - categorical_accuracy: 0.6886 - val_loss: 1.2351 - val_categorical_accuracy: 0.6951\n",
            "Epoch 4/10\n",
            "19350/19350 [==============================] - 10s 513us/step - loss: 0.9094 - categorical_accuracy: 0.6828 - val_loss: 1.4163 - val_categorical_accuracy: 0.6033\n",
            "Epoch 5/10\n",
            "19350/19350 [==============================] - 10s 513us/step - loss: 0.7727 - categorical_accuracy: 0.7099 - val_loss: 1.4984 - val_categorical_accuracy: 0.6124\n",
            "Epoch 6/10\n",
            "19350/19350 [==============================] - 10s 515us/step - loss: 0.5951 - categorical_accuracy: 0.7761 - val_loss: 2.0326 - val_categorical_accuracy: 0.8277\n",
            "Epoch 7/10\n",
            "19350/19350 [==============================] - 10s 524us/step - loss: 0.4341 - categorical_accuracy: 0.7705 - val_loss: 2.7199 - val_categorical_accuracy: 0.7580\n",
            "Epoch 8/10\n",
            "19350/19350 [==============================] - 10s 516us/step - loss: 0.3265 - categorical_accuracy: 0.8368 - val_loss: 3.1300 - val_categorical_accuracy: 0.6754\n",
            "Epoch 9/10\n",
            "19350/19350 [==============================] - 10s 522us/step - loss: 0.2511 - categorical_accuracy: 0.8237 - val_loss: 3.3797 - val_categorical_accuracy: 0.7842\n",
            "Epoch 10/10\n",
            "19350/19350 [==============================] - 10s 521us/step - loss: 0.2000 - categorical_accuracy: 0.8261 - val_loss: 4.6096 - val_categorical_accuracy: 0.8408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLY5LLDu-aGd",
        "colab_type": "text"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxSwb0CM_Gnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(model, x, x_raw, n_classes):\n",
        "  y_hat = model.predict(x)\n",
        "  y_hat = y_hat.reshape(-1, n_classes).argmax(axis=1).reshape(x.shape[:2])\n",
        "  labels = []\n",
        "  for row in x_raw.itertuples():\n",
        "    sent_idx = row.Index - 1\n",
        "    for tok_idx in range(row.n_toks):\n",
        "      if y_hat[sent_idx][tok_idx] == 0:\n",
        "        label = \"O\"\n",
        "      elif y_hat[sent_idx][tok_idx] == 1:\n",
        "        label = \"I\"\n",
        "      else:\n",
        "        label = \"B\"\n",
        "      labels.append(label)\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLVudMQZBm_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_hat = get_predictions(model, dev_x, dev_raw, config.N_CLASSES)\n",
        "result_df = pd.concat([dev_df, pd.DataFrame(y_hat, columns=['label'])],\n",
        "                      axis=1, sort=False)\n",
        "print(result_df['label'].value_counts())\n",
        "result_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt4x3pQNaEP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def si_predictions_to_spans(label_df):\n",
        "  spans = []\n",
        "  prev_label = 'O'\n",
        "  prev_span_start = '-1'\n",
        "  prev_span_end = '-1'\n",
        "  prev_article = ''\n",
        "\n",
        "  first_line = True\n",
        "  for row in label_df.itertuples():\n",
        "    article = row.document_id\n",
        "    span_start = row.token_start\n",
        "    span_end = row.token_end\n",
        "    label = row.label\n",
        "\n",
        "    span, prev_span_start = update_prediction(article, label,\n",
        "                                              span_start, span_end,\n",
        "                                              prev_article, prev_label,\n",
        "                                              prev_span_start,\n",
        "                                              prev_span_end)\n",
        "    if span is not None:\n",
        "      spans.append(span)\n",
        "\n",
        "    prev_article = article\n",
        "    prev_label = label\n",
        "    prev_span_end = span_end\n",
        "\n",
        "  # Make sure we get the last prediction\n",
        "  span, _ = update_prediction(article, label, span_start, span_end,\n",
        "                              prev_article, prev_label, prev_span_start,\n",
        "                              prev_span_end)\n",
        "  if span is not None:\n",
        "    spans.append(span)\n",
        "  return spans\n",
        "\n",
        "# Helper method for si_predictions_to_spans\n",
        "def update_prediction(article, label, span_start, span_end,\n",
        "                     prev_article, prev_label, prev_span_start, prev_span_end):\n",
        "  span = None\n",
        "  cur_span_start = prev_span_start\n",
        "  # Ending a span: I-O, B-O, I-B, B-B, new article\n",
        "  if prev_label != 'O' and (label != 'I' or prev_article != article):\n",
        "    span = (prev_article, prev_span_start, prev_span_end)\n",
        "\n",
        "  # Starting a new span: O-B, O-I, I-B, B-B, new article\n",
        "  if label == 'B' or (label == 'I' and prev_label == 'O') \\\n",
        "          or prev_article != article:\n",
        "      # Update the start of the current label span\n",
        "      cur_span_start = span_start\n",
        "\n",
        "  return span, cur_span_start"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCKd5VK7SswK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spans = si_predictions_to_spans(result_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9fMlY7BMyWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "now = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
        "outfile = 'spans_' + now + '.txt'\n",
        "logfile = 'log_' + now + '.txt'\n",
        "\n",
        "with open(logfile, mode='w') as f:\n",
        "  f.write('DATA PREPROCESSING\\n\\n')\n",
        "  for comment in comments:\n",
        "    comment = comment.decode(\"utf-8\")\n",
        "    comment = comment.replace('#', '')\n",
        "    fields = comment.split(',')\n",
        "    for field in fields:\n",
        "      f.write(comment.strip() + '\\n')\n",
        "  f.write('\\n\\nCONFIG\\n\\n')\n",
        "  f.write('max seq len: ' + str(config.MAX_SEQ_LEN) + '\\n')\n",
        "  f.write('embedding depth: ' + str(config.EMBED_DIM) + '\\n')\n",
        "  f.write('batch size: ' + str(config.BATCH_SIZE) + '\\n')\n",
        "  f.write('epochs: ' + str(config.EPOCHS) + '\\n')\n",
        "  f.write('number of labels: ' + str(config.N_CLASSES) + '\\n')\n",
        "  f.write('O weight: ' + str(config.O_WEIGHT) +\n",
        "          ', I weight:' + str(config.I_WEIGHT) +\n",
        "          ', B weight: ' + str(config.B_WEIGHT) + '\\n')\n",
        "  f.write('hidden units: ' + str(config.LSTM_UNITS) + '\\n')\n",
        "  f.write('dropout rate: ' + str(config.DROPOUT) + '\\n')\n",
        "  f.write('optimizer: ' + config.OPTIMIZER + '\\n')\n",
        "  f.write('metric: ' + config.METRIC + '\\n')\n",
        "  f.write('loss: ' + config.LOSS + '\\n')\n",
        "  f.write('\\n\\nMODEL HISTORY\\n\\n')\n",
        "  f.write('Validation loss ' + config.LOSS + '\\n')\n",
        "  f.write(str(history.history['val_loss']) + '\\n')\n",
        "  f.write('Loss ' + config.LOSS + '\\n')\n",
        "  f.write(str(history.history['loss']) + '\\n')\n",
        "  f.write('Validation ' + config.METRIC + '\\n')\n",
        "  f.write(str(history.history['val_' + config.METRIC]) + '\\n')\n",
        "  f.write(config.METRIC + '\\n')\n",
        "  f.write(str(history.history[config.METRIC]) + '\\n')\n",
        "  f.write('\\n\\nMODEL SUMMARY\\n\\n')\n",
        "  model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "with open(outfile, mode='w') as f:\n",
        "  for span in spans:\n",
        "    f.write(str(span[0]) + '\\t' + str(span[1]) + '\\t' + str(span[2]) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}