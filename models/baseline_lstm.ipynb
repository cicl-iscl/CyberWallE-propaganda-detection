{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline-lstm",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VubK6qZQUmeM",
        "colab_type": "text"
      },
      "source": [
        "# Challenges in NLP, WS19/20\n",
        "\n",
        "Blaschke Verena, ISCL MA<br/>\n",
        "Korniyenko Maxim, ISCL MA<br/>\n",
        "Tureski Sam, ML MA<br/>\n",
        "\n",
        "-----\n",
        "## Baseline model for Span Identification task\n",
        "-----\n",
        "\n",
        "The working process looks like the following:\n",
        "- Data preparation.\n",
        "- Creating the model.\n",
        "- Training the model.\n",
        "- Testing the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddQhWcO8_jIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSkNNNrI9ZxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "from enum import Enum\n",
        "from itertools import takewhile\n",
        "import urllib.request\n",
        "import time\n",
        "\n",
        "# Creating the model\n",
        "from keras.layers import Bidirectional, CuDNNLSTM, Dense, Dropout, TimeDistributed\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Results analysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlVCNsD2-rfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# installing tools for oversampling\n",
        "# !pip install -U imbalanced-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7UDJPQ3JLeL",
        "colab_type": "text"
      },
      "source": [
        "# Processing the Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9px5zsuDQt5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper method for prepare_data\n",
        "def get_comments(filename, url=True):\n",
        "    if url:\n",
        "        comments = []\n",
        "        with urllib.request.urlopen(filename) as f:\n",
        "            for line in f:\n",
        "                if line.startswith(b'#'):\n",
        "                    comments.append(line.decode(\"utf-8\"))\n",
        "                else:\n",
        "                    break\n",
        "        return comments\n",
        "    with open(filename, 'r', encoding='utf8') as f:\n",
        "        commentiter = takewhile(lambda s: s.startswith('#'), f)\n",
        "        comments = list(commentiter)\n",
        "    return comments\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def get_cols(input_df, col):\n",
        "    return input_df.groupby('sent_id')[col].apply(list).to_frame()\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def add_sent_lens(input_df, col='token'):\n",
        "    input_df['n_toks'] = input_df[col].apply(lambda x: len(x))\n",
        "    return input_df\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def get_features(input_df, feature_cols):\n",
        "    x = add_sent_lens(get_cols(input_df, 'token'))\n",
        "    for feature in feature_cols:\n",
        "        x = pd.merge(left=x, right=get_cols(input_df, feature),\n",
        "                     left_on='sent_id', right_on='sent_id')\n",
        "    return x\n",
        "\n",
        "\n",
        "def encode_x(x, word2embedding, feature_header, max_seq_len, embed_dim):\n",
        "    \"\"\"Encode the input data.\n",
        "\n",
        "    Arguments:\n",
        "    x -- a Pandas dataframe\n",
        "    word2embedding -- a dict(str -> np.array) from tokens to embeddings\n",
        "    feature_header -- dataframe names of additional feature columns\n",
        "    max_seq_len -- the maximum number of tokens per sentence in x\n",
        "    embed_dim -- the array length of the vectors in word2embedding\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros([len(x), max_seq_len,\n",
        "                                 embed_dim + len(feature_header)])\n",
        "    for row in x.itertuples():\n",
        "        sent_idx = row.Index - 1\n",
        "        for tok_idx in range(row.n_toks):\n",
        "            word = row.token[tok_idx]\n",
        "            embedding_matrix[sent_idx][tok_idx][:embed_dim] = \\\n",
        "                word2embedding.get(word, np.random.randn(embed_dim))\n",
        "            for i, feature in enumerate(feature_header):\n",
        "                embedding_matrix[sent_idx][tok_idx][embed_dim + i] = \\\n",
        "                    getattr(row, feature)[tok_idx]\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def encode_y(y, label2idx, max_seq_len, n_classes):\n",
        "    if n_classes == 1:\n",
        "        labels = np.zeros([len(y), max_seq_len])\n",
        "    else:\n",
        "        labels = np.zeros([len(y), max_seq_len, n_classes])\n",
        "\n",
        "    for row in y.itertuples():\n",
        "        sent_idx = row.Index - 1\n",
        "        for tok_idx, label in enumerate(row.label):\n",
        "            labels[sent_idx][tok_idx] = label2idx[label]\n",
        "    return labels\n",
        "\n",
        "\n",
        "def prepare_data(config, word2embedding, training):\n",
        "    # We're getting the comments this way so we can:\n",
        "    # - add them to the output\n",
        "    # - parse lines that actually contain '#' as token\n",
        "    if training:\n",
        "        infile = config.TRAIN_URL\n",
        "    else:\n",
        "        infile = config.DEV_URL\n",
        "    comments = get_comments(infile, config.ONLINE_SOURCES)\n",
        "    df = pd.read_csv(infile, sep='\\t', skiprows=len(comments), quoting=3)\n",
        "\n",
        "    std_cols = ['document_id', 'sent_id', 'token_start',\n",
        "                'token_end', 'token', 'label']\n",
        "    feature_cols = []\n",
        "    for col in df.columns:\n",
        "        if col not in std_cols:\n",
        "            feature_cols.append(col)\n",
        "\n",
        "    x_raw = get_features(df, feature_cols)\n",
        "    x_enc = encode_x(x_raw, word2embedding, feature_cols,\n",
        "                     config.MAX_SEQ_LEN, config.EMBED_DIM)\n",
        "\n",
        "    y = None\n",
        "    sample_weight = None\n",
        "    if 'label' in df.columns:\n",
        "        y_raw = get_cols(df, 'label')\n",
        "        if config.N_CLASSES == 3:\n",
        "            label2idx = {\"O\": [1, 0, 0], \"B\": [0, 0, 1], \"I\": [0, 1, 0]}\n",
        "        elif config.N_CLASSES == 2:\n",
        "            label2idx = {\"O\": [1, 0], \"B\": [0, 1], \"I\": [0, 1]}\n",
        "        y = encode_y(y_raw, label2idx, config.MAX_SEQ_LEN, config.N_CLASSES)\n",
        "        label2weight = {'O': config.O_WEIGHT, 'I': config.I_WEIGHT,\n",
        "                        'B': config.B_WEIGHT}\n",
        "        sample_weight = encode_y(y_raw, label2weight, config.MAX_SEQ_LEN,\n",
        "                                 n_classes=1)\n",
        "\n",
        "    return df, x_raw, x_enc, y, sample_weight, comments\n",
        "\n",
        "def get_data(config, word2embedding=None):\n",
        "    if not word2embedding:\n",
        "        word2embedding = {}\n",
        "        f = open(config.EMBEDDING_PATH)\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word2embedding[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "        f.close()\n",
        "\n",
        "    _, _, train_x, train_y, sample_weight, comments = prepare_data(\n",
        "        config, word2embedding, training=True)\n",
        "    dev_df, dev_raw, dev_x, _, _, _ = prepare_data(config, word2embedding,\n",
        "                                                   training=False)\n",
        "    return Data(train_x, train_y, sample_weight, comments,\n",
        "                dev_df, dev_raw, dev_x)\n",
        "\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, train_x, train_y, sample_weight,\n",
        "                 comments, dev_df, dev_raw, dev_x):\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.sample_weight = sample_weight\n",
        "        self.comments = comments\n",
        "        self.dev_df = dev_df\n",
        "        self.dev_raw = dev_raw\n",
        "        self.dev_x = dev_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HboCSN-CJPrH",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gxxeBUClDOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def custom_loss(y_true, y_pred):\n",
        "#   # for test purposes\n",
        "#   return K.variable(value=np.ones(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuzEvEcGI1xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bilstm(input_shape, config):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(CuDNNLSTM(config.LSTM_UNITS,\n",
        "                                      return_sequences=True),\n",
        "                            input_shape=input_shape))\n",
        "    model.add(Dropout(config.DROPOUT))\n",
        "    model.add(TimeDistributed(Dense(config.N_CLASSES, activation='softmax')))\n",
        "    model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER,\n",
        "                  metrics=[config.METRIC], sample_weight_mode='temporal')\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_and_fit_bilstm(config, train_x, train_y, sample_weight):\n",
        "    model = get_bilstm(train_x.shape[1:], config)\n",
        "    history = model.fit(train_x, train_y, epochs=config.EPOCHS,\n",
        "                        batch_size=config.BATCH_SIZE, validation_split=0.1,\n",
        "                        sample_weight=sample_weight, verbose=1,)\n",
        "    return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLY5LLDu-aGd",
        "colab_type": "text"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxSwb0CM_Gnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bio_predictions(model, x, x_raw, n_classes):\n",
        "    y_hat = model.predict(x)\n",
        "    y_hat = y_hat.reshape(-1, n_classes).argmax(axis=1).reshape(x.shape[:2])\n",
        "    labels = []\n",
        "    for row in x_raw.itertuples():\n",
        "        sent_idx = row.Index - 1\n",
        "        for tok_idx in range(row.n_toks):\n",
        "            if y_hat[sent_idx][tok_idx] == 0:\n",
        "                label = \"O\"\n",
        "            elif y_hat[sent_idx][tok_idx] == 1:\n",
        "                label = \"I\"\n",
        "            else:\n",
        "                label = \"B\"\n",
        "            labels.append(label)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def si_predictions_to_spans(label_df):\n",
        "    spans = []\n",
        "    prev_label = 'O'\n",
        "    prev_span_start = '-1'\n",
        "    prev_span_end = '-1'\n",
        "    prev_article = ''\n",
        "\n",
        "    for row in label_df.itertuples():\n",
        "        article = row.document_id\n",
        "        span_start = row.token_start\n",
        "        span_end = row.token_end\n",
        "        label = row.label\n",
        "\n",
        "        span, prev_span_start = update_prediction(article, label,\n",
        "                                                  span_start, span_end,\n",
        "                                                  prev_article, prev_label,\n",
        "                                                  prev_span_start,\n",
        "                                                  prev_span_end)\n",
        "        if span is not None:\n",
        "            spans.append(span)\n",
        "\n",
        "        prev_article = article\n",
        "        prev_label = label\n",
        "        prev_span_end = span_end\n",
        "\n",
        "    # Make sure we get the last prediction\n",
        "    span, _ = update_prediction(article, label, span_start, span_end,\n",
        "                                prev_article, prev_label, prev_span_start,\n",
        "                                prev_span_end)\n",
        "    if span is not None:\n",
        "        spans.append(span)\n",
        "    return spans\n",
        "\n",
        "\n",
        "# Helper method for si_predictions_to_spans\n",
        "def update_prediction(article, label, span_start, span_end, prev_article,\n",
        "                      prev_label, prev_span_start, prev_span_end):\n",
        "    span = None\n",
        "    cur_span_start = prev_span_start\n",
        "    # Ending a span: I-O, B-O, I-B, B-B, new article\n",
        "    if prev_label != 'O' and (label != 'I' or prev_article != article):\n",
        "        span = (prev_article, prev_span_start, prev_span_end)\n",
        "\n",
        "    # Starting a new span: O-B, O-I, I-B, B-B, new article\n",
        "    if label == 'B' or (label == 'I' and prev_label == 'O') \\\n",
        "            or prev_article != article:\n",
        "        # Update the start of the current label span\n",
        "        cur_span_start = span_start\n",
        "\n",
        "    return span, cur_span_start\n",
        "\n",
        "\n",
        "def predict(config, model, history, dev_df, dev_raw, dev_x, comments,\n",
        "            file_prefix, file_stem, file_suffix):\n",
        "    y_hat = get_bio_predictions(model, dev_x, dev_raw, config.N_CLASSES)\n",
        "    result_df = pd.concat([dev_df, pd.DataFrame(y_hat, columns=['label'])],\n",
        "                          axis=1, sort=False)\n",
        "    spans = si_predictions_to_spans(result_df)\n",
        "\n",
        "    outfile = file_prefix + 'spans_' + file_stem + '_' + file_suffix + '.txt'\n",
        "    logfile = file_prefix + 'log_' + file_stem + '_' + file_suffix + '.txt'\n",
        "\n",
        "    with open(logfile, mode='w') as f:\n",
        "        f.write('DATA PREPROCESSING\\n\\n')\n",
        "        for comment in comments:\n",
        "            comment = comment.replace('#', '')\n",
        "            fields = comment.split(',')\n",
        "            for field in fields:\n",
        "                f.write(comment.strip() + '\\n')\n",
        "        f.write('\\n\\nCONFIG\\n\\n')\n",
        "        f.write(config.pretty_str())\n",
        "        f.write('\\n\\nMODEL HISTORY\\n\\n')\n",
        "        f.write('Validation loss ' + config.LOSS + '\\n')\n",
        "        f.write(str(history.history['val_loss']) + '\\n')\n",
        "        f.write('Loss ' + config.LOSS + '\\n')\n",
        "        f.write(str(history.history['loss']) + '\\n')\n",
        "        f.write('Validation ' + config.METRIC + '\\n')\n",
        "        f.write(str(history.history['val_' + config.METRIC]) + '\\n')\n",
        "        f.write(config.METRIC + '\\n')\n",
        "        f.write(str(history.history[config.METRIC]) + '\\n')\n",
        "        f.write('\\n\\nMODEL SUMMARY\\n\\n')\n",
        "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "    with open(outfile, mode='w') as f:\n",
        "        for span in spans:\n",
        "            f.write(str(span[0]) + '\\t' + str(span[1]) + '\\t' +\n",
        "                    str(span[2]) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhhDEJEpSvOD",
        "colab_type": "text"
      },
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AMwMQ3TVLUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(config, file_stem, file_suffix, verbose=True,\n",
        "        data=None, word2embedding=None, file_prefix=''):\n",
        "    if verbose:\n",
        "        print('Running with config:')\n",
        "        print(config.pretty_str())\n",
        "    if not data:\n",
        "        if verbose:\n",
        "            print('Encoding the data')\n",
        "        data = get_data(config, word2embedding)\n",
        "    if verbose:\n",
        "        print('Building the model')\n",
        "    model, history = create_and_fit_bilstm(config, data.train_x,\n",
        "                                           data.train_y,\n",
        "                                           data.sample_weight)\n",
        "    if verbose:\n",
        "        print('Predicting the test data spans')\n",
        "    predict(config, model, history, data.dev_df, data.dev_raw,\n",
        "            data.dev_x, data.comments, file_prefix, file_stem, file_suffix)\n",
        "    if verbose:\n",
        "        print('Done!\\n\\n')\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjw0IqAaaJ9g",
        "colab_type": "text"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eANZq-VcaEcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:\n",
        "    def __init__(self, args=None):\n",
        "        \"\"\"Creates a default configuration.\n",
        "\n",
        "        Keyword arguments:\n",
        "        args -- a dict(str -> ?) containing values diverging from the default\n",
        "        \"\"\"\n",
        "        # Encoding the data:\n",
        "        self.MAX_SEQ_LEN = 35\n",
        "        self.EMBED_DIM = 100\n",
        "        self.N_CLASSES = 2\n",
        "        self.ONLINE_SOURCES = True\n",
        "        self.TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/train-data-improved-sentiwordnet-arguingfull.tsv?token=AD7GEDOZHQ7BVKUTD7RZYJS6AOSVW'\n",
        "        self.DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/dev-improved-sentiwordnet-arguingfull.tsv?token=AD7GEDN7YPFXYH5TSRBSNKS6AOSWK'\n",
        "        self.EMBEDDING_PATH = 'gdrive/My Drive/colab_projects/data/glove.6B.100d.txt'\n",
        "\n",
        "        # Building the model:\n",
        "        self.BATCH_SIZE = 128\n",
        "        self.EPOCHS = 10\n",
        "        self.O_WEIGHT = 1.0\n",
        "        self.I_WEIGHT = 6.5\n",
        "        self.B_WEIGHT = 6.5\n",
        "        self.LSTM_UNITS = 512\n",
        "        self.DROPOUT = 0.25\n",
        "        self.OPTIMIZER = 'adam'\n",
        "        self.METRIC = 'categorical_accuracy'\n",
        "        self.LOSS = 'categorical_crossentropy'\n",
        "\n",
        "        if args:\n",
        "            for key in args:\n",
        "                setattr(self, key, args[key])\n",
        "\n",
        "    def pretty_str(self):\n",
        "        return 'max seq len: ' + str(self.MAX_SEQ_LEN) + '\\n' + \\\n",
        "               'embedding depth: ' + str(self.EMBED_DIM) + '\\n' + \\\n",
        "               'number of labels: ' + str(config.N_CLASSES) + '\\n' + \\\n",
        "               'batch size: ' + str(self.BATCH_SIZE) + '\\n' + \\\n",
        "               'epochs: ' + str(self.EPOCHS) + '\\n' + \\\n",
        "               'O weight: ' + str(self.O_WEIGHT) + \\\n",
        "               ', I weight:' + str(self.I_WEIGHT) + \\\n",
        "               ', B weight: ' + str(self.B_WEIGHT) + '\\n' + \\\n",
        "               'hidden units: ' + str(self.LSTM_UNITS) + '\\n' + \\\n",
        "               'dropout rate: ' + str(self.DROPOUT) + '\\n' + \\\n",
        "               'optimizer: ' + self.OPTIMIZER + '\\n' + \\\n",
        "               'metric: ' + self.METRIC + '\\n' + \\\n",
        "               'loss: ' + self.LOSS + '\\n'\n",
        "\n",
        "def run_config(config, file_prefix, data=None, repetitions=5, verbose=True):\n",
        "    now = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
        "    for i in range(repetitions):\n",
        "        if verbose:\n",
        "            print(\"Iteration \" + str(i + 1) + \" of \" + str(repetitions))\n",
        "        data = run(config, data=data, verbose=verbose,\n",
        "                   file_prefix=file_prefix, file_stem=now, file_suffix=str(i))\n",
        "    # Return data in case the next config only changes model features\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbk7dN2xU_8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_prefix = '/content/gdrive/My Drive/semeval-predictions/'\n",
        "data = None\n",
        "# config = Config()\n",
        "config = Config({'TRAIN_URL': 'train-data-improved-sentiwordnet-arguingfull-pos.tsv',\n",
        "                 'DEV_URL': 'dev-improved-sentiwordnet-arguingfull-pos.tsv',\n",
        "                 'ONLINE_SOURCES': False})\n",
        "data = run_config(config, file_prefix, data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}