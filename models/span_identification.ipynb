{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "span_identification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VubK6qZQUmeM",
        "colab_type": "text"
      },
      "source": [
        "# Challenges in Computational Linguistics, WS 19/20\n",
        "\n",
        "Blaschke Verena, ISCL MA<br/>\n",
        "Korniyenko Maxim, ISCL MA<br/>\n",
        "Tureski Sam, ML MA<br/>\n",
        "\n",
        "-----\n",
        "## SemEval2020-11: Propaganda Detection\n",
        "### Task 1: Span identification\n",
        "-----\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-5HfEvv3STwo",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_1W_TMlZXll",
        "colab_type": "text"
      },
      "source": [
        "# model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uUPU9_5SZ9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import takewhile\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from keras.layers import Bidirectional, CuDNNLSTM, Dense, Dropout, \\\n",
        "    TimeDistributed\n",
        "from keras.models import Sequential\n",
        "\n",
        "########################\n",
        "# Processing the input #\n",
        "########################\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def get_comments(filename, url=True):\n",
        "    if url:\n",
        "        comments = []\n",
        "        with urllib.request.urlopen(filename) as f:\n",
        "            for line in f:\n",
        "                if line.startswith(b'#'):\n",
        "                    comments.append(line.decode(\"utf-8\"))\n",
        "                else:\n",
        "                    break\n",
        "        return comments\n",
        "    with open(filename, 'r', encoding='utf8') as f:\n",
        "        commentiter = takewhile(lambda s: s.startswith('#'), f)\n",
        "        comments = list(commentiter)\n",
        "    return comments\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def get_cols(input_df, col):\n",
        "    return input_df.groupby('sent_id')[col].apply(list).to_frame()\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def add_sent_lens(input_df, col='token'):\n",
        "    input_df['n_toks'] = input_df[col].apply(lambda x: len(x))\n",
        "    return input_df\n",
        "\n",
        "\n",
        "# Helper method for prepare_data\n",
        "def get_features(input_df, feature_cols):\n",
        "    x = add_sent_lens(get_cols(input_df, 'token'))\n",
        "    for feature in feature_cols:\n",
        "        x = pd.merge(left=x, right=get_cols(input_df, feature),\n",
        "                     left_on='sent_id', right_on='sent_id')\n",
        "    return x\n",
        "\n",
        "\n",
        "def bert_embeddings_for_sent(bert_tokens, row, feature_header, embedding_matrix,\n",
        "                             embed_dim, sent_idx, uncased):\n",
        "    if len(bert_tokens) < len(row.token):\n",
        "        print('BERT', [i[0] for i in bert_tokens])\n",
        "        print('X', row.token)\n",
        "    word_idx = 0\n",
        "    for (tok, embed) in bert_tokens:\n",
        "        if word_idx == row.n_toks:\n",
        "            break\n",
        "        word = str(row.token[word_idx])\n",
        "        if word == '\\ufeff':\n",
        "            word_idx += 1\n",
        "            continue\n",
        "        if uncased:\n",
        "            word = word.lower()\n",
        "        if tok == word or word.startswith(tok):\n",
        "            # startswith: Use embedding of first subtoken\n",
        "            embedding_matrix[sent_idx - 1][word_idx][:embed_dim] = embed\n",
        "            for i, feature in enumerate(feature_header):\n",
        "                embedding_matrix[sent_idx - 1][word_idx][embed_dim + i] = \\\n",
        "                    getattr(row, feature)[word_idx]\n",
        "            word_idx += 1\n",
        "            continue\n",
        "        if tok.startswith('##') and not word.startswith('##'):\n",
        "            # BERT word continutation prefix (e.g. per ##pet ##uate)\n",
        "            continue\n",
        "\n",
        "\n",
        "def encode_x_bert(x, bert_file, feature_header, max_seq_len, embed_dim=768, uncased=True):\n",
        "    # TODO this currently assumes that the BERT file only contains information\n",
        "    # about a single layer. extend this to multiple layers?\n",
        "    embedding_matrix = np.zeros([len(x), max_seq_len,\n",
        "                                 embed_dim + len(feature_header)])\n",
        "    prev_sent_idx = 1\n",
        "    bert_tokens = []\n",
        "    sentences = x.itertuples()\n",
        "    with open(bert_file, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            cells = line.split('\\t')\n",
        "            sent_idx = int(cells[0])\n",
        "            layer = int(cells[1])\n",
        "            token = cells[2]\n",
        "            embedding = np.fromstring(cells[3][1:-1], sep=',')\n",
        "\n",
        "            if sent_idx != prev_sent_idx:\n",
        "                if sent_idx % 1000 == 0:\n",
        "                    print(\"BERT embeddings for sentence\", sent_idx)\n",
        "                row = next(sentences)\n",
        "                assert row.Index == prev_sent_idx\n",
        "                bert_embeddings_for_sent(bert_tokens, row, feature_header,\n",
        "                                         embedding_matrix, embed_dim,\n",
        "                                         prev_sent_idx, uncased)\n",
        "                bert_tokens = []\n",
        "\n",
        "            bert_tokens.append((token, embedding))\n",
        "            prev_sent_idx = sent_idx\n",
        "\n",
        "    # Last line:\n",
        "    row = next(sentences)\n",
        "    bert_embeddings_for_sent(bert_tokens, row, feature_header, embedding_matrix,\n",
        "                             embed_dim, prev_sent_idx, uncased)\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def encode_x(x, word2embedding, feature_header, max_seq_len,\n",
        "             embed_dim, uncased):\n",
        "    \"\"\"Encode the input data.\n",
        "\n",
        "    Arguments:\n",
        "    x -- a Pandas dataframe\n",
        "    word2embedding -- a dict(str -> np.array) from tokens to embeddings\n",
        "    feature_header -- dataframe names of additional feature columns\n",
        "    max_seq_len -- the maximum number of tokens per sentence in x\n",
        "    embed_dim -- the array length of the vectors in word2embedding\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros([len(x), max_seq_len,\n",
        "                                 embed_dim + len(feature_header)])\n",
        "    for row in x.itertuples():\n",
        "        sent_idx = row.Index - 1\n",
        "        for tok_idx in range(row.n_toks):\n",
        "            word = str(row.token[tok_idx])\n",
        "            if uncased:\n",
        "                word = word.lower()\n",
        "            embedding_matrix[sent_idx][tok_idx][:embed_dim] = \\\n",
        "                word2embedding.get(word, np.random.randn(embed_dim))\n",
        "            for i, feature in enumerate(feature_header):\n",
        "                embedding_matrix[sent_idx][tok_idx][embed_dim + i] = \\\n",
        "                    getattr(row, feature)[tok_idx]\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def encode_y(y, label2idx, max_seq_len, n_classes):\n",
        "    if n_classes == 1:\n",
        "        labels = np.zeros([len(y), max_seq_len])\n",
        "    else:\n",
        "        labels = np.zeros([len(y), max_seq_len, n_classes])\n",
        "\n",
        "    for row in y.itertuples():\n",
        "        sent_idx = row.Index - 1\n",
        "        for tok_idx, label in enumerate(row.label):\n",
        "            labels[sent_idx][tok_idx] = label2idx[label]\n",
        "    return labels\n",
        "\n",
        "\n",
        "def prepare_data(config, word2embedding, training):\n",
        "    # We're getting the comments this way so we can:\n",
        "    # - add them to the output\n",
        "    # - parse lines that actually contain '#' as token\n",
        "    if training:\n",
        "        infile = config.TRAIN_URL\n",
        "    else:\n",
        "        infile = config.DEV_URL\n",
        "    comments = get_comments(infile, config.ONLINE_SOURCES)\n",
        "    df = pd.read_csv(infile, sep='\\t', skiprows=len(comments), quoting=3,\n",
        "                     encoding='utf8')\n",
        "\n",
        "    std_cols = ['document_id', 'sent_id', 'token_start',\n",
        "                'token_end', 'token', 'label']\n",
        "    feature_cols = []\n",
        "    for col in df.columns:\n",
        "        if col not in std_cols:\n",
        "            feature_cols.append(col)\n",
        "\n",
        "    x_raw = get_features(df, feature_cols)\n",
        "    if config.USE_BERT:\n",
        "        if training:\n",
        "            bert_file = config.TRAIN_BERT\n",
        "        else:\n",
        "            bert_file = config.DEV_BERT\n",
        "        x_enc = encode_x_bert(x_raw, bert_file, feature_cols,\n",
        "                              config.MAX_SEQ_LEN, config.EMBED_DIM,\n",
        "                              config.UNCASED)\n",
        "    else:\n",
        "        x_enc = encode_x(x_raw, word2embedding, feature_cols,\n",
        "                     config.MAX_SEQ_LEN, config.EMBED_DIM, config.UNCASED)\n",
        "\n",
        "    y = None\n",
        "    sample_weight = None\n",
        "    if 'label' in df.columns:\n",
        "        y_raw = get_cols(df, 'label')\n",
        "        if config.N_CLASSES == 3:\n",
        "            label2idx = {\"O\": [1, 0, 0], \"B\": [0, 0, 1], \"I\": [0, 1, 0]}\n",
        "        elif config.N_CLASSES == 2:\n",
        "            label2idx = {\"O\": [1, 0], \"B\": [0, 1], \"I\": [0, 1]}\n",
        "        y = encode_y(y_raw, label2idx, config.MAX_SEQ_LEN, config.N_CLASSES)\n",
        "        label2weight = {'O': config.O_WEIGHT, 'I': config.I_WEIGHT,\n",
        "                        'B': config.B_WEIGHT}\n",
        "        sample_weight = encode_y(y_raw, label2weight, config.MAX_SEQ_LEN,\n",
        "                                 n_classes=1)\n",
        "\n",
        "    return df, x_raw, x_enc, y, sample_weight, comments\n",
        "\n",
        "\n",
        "def load_zipped_embeddings(infile):\n",
        "    word2embedding = {}\n",
        "    with zipfile.ZipFile(infile) as f_in_zip:\n",
        "        file_in = f_in_zip.filelist[0].filename\n",
        "        i = 0\n",
        "        with f_in_zip.open(file_in, 'r') as f_in:\n",
        "            for line in f_in:\n",
        "                values = line.decode().rstrip().split()\n",
        "                word2embedding[values[0]] = np.asarray(values[1:],\n",
        "                                                       dtype='float32')\n",
        "                i += 1\n",
        "                if i % 100000 == 0:\n",
        "                    print(\"Read \" + str(i) + \" embeddings\")\n",
        "    return word2embedding\n",
        "\n",
        "\n",
        "def get_data(config, word2embedding=None):\n",
        "    if (not word2embedding) and (not config.USE_BERT):\n",
        "        if config.EMBEDDING_PATH[-4:] == '.zip':\n",
        "            word2embedding = load_zipped_embeddings(config.EMBEDDING_PATH)\n",
        "        else:\n",
        "            word2embedding = {}\n",
        "            f = open(config.EMBEDDING_PATH)\n",
        "            for line in f:\n",
        "                values = line.rstrip().split()\n",
        "                word2embedding[values[0]] = np.asarray(values[1:],\n",
        "                                                       dtype='float32')\n",
        "            f.close()\n",
        "\n",
        "    _, _, train_x, train_y, sample_weight, comments = prepare_data(\n",
        "        config, word2embedding, training=True)\n",
        "    dev_df, dev_raw, dev_x, _, _, _ = prepare_data(config, word2embedding,\n",
        "                                                   training=False)\n",
        "    return Data(train_x, train_y, sample_weight, comments,\n",
        "                dev_df, dev_raw, dev_x)\n",
        "\n",
        "\n",
        "class Data:\n",
        "    def __init__(self,\n",
        "                 # If initializing on the fly:\n",
        "                 train_x=None, train_y=None, sample_weight=None,\n",
        "                 comments=None, dev_df=None, dev_raw=None, dev_x=None,\n",
        "                 # If initializing from files:\n",
        "                 path=None):\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.sample_weight = sample_weight\n",
        "        self.comments = comments\n",
        "        self.dev_df = dev_df\n",
        "        self.dev_raw = dev_raw\n",
        "        self.dev_x = dev_x\n",
        "        if path:\n",
        "            self.load(path)\n",
        "\n",
        "\n",
        "    def save(self, path='gdrive/My Drive/colab_projects/data/data/'):\n",
        "        np.save(path + 'train_x', self.train_x)\n",
        "        np.save(path + 'train_y', self.train_y)\n",
        "        np.save(path + 'dev_x', self.dev_x)\n",
        "        np.save(path + 'sample_weight', self.sample_weight)\n",
        "        self.dev_raw.to_csv(path + 'dev_raw')\n",
        "        self.dev_df.to_csv(path + 'dev_df')\n",
        "        with open(path + 'comments.txt', 'w', encoding='utf8') as f:\n",
        "            for comment in self.comments:\n",
        "                f.write(comment + '\\n')\n",
        "\n",
        "\n",
        "    def load(self, path='gdrive/My Drive/colab_projects/data/data/'):\n",
        "        self.train_x = np.load(path + 'train_x.npy')\n",
        "        self.train_y = np.load(path + 'train_y.npy')\n",
        "        self.dev_x = np.load(path + 'dev_x.npy')\n",
        "        self.sample_weight = np.load(path + 'sample_weight.npy')\n",
        "        self.dev_raw = pd.read_csv(path + 'dev_raw')\n",
        "        self.dev_df = pd.read_csv(path + 'dev_df')\n",
        "        self.comments =[]\n",
        "        with open(path + 'comments.txt', 'r', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    self.comments.append(line)\n",
        "\n",
        "\n",
        "######################\n",
        "# Creating the model #\n",
        "######################\n",
        "\n",
        "\n",
        "def get_bilstm(input_shape, config):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(CuDNNLSTM(config.LSTM_UNITS,\n",
        "                                      return_sequences=True),\n",
        "                            input_shape=input_shape))\n",
        "    model.add(Dropout(config.DROPOUT))\n",
        "    model.add(TimeDistributed(Dense(config.N_CLASSES, activation='softmax')))\n",
        "    model.compile(loss=config.LOSS, optimizer=config.OPTIMIZER,\n",
        "                  metrics=[config.METRIC], sample_weight_mode='temporal')\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_and_fit_bilstm(config, train_x, train_y, sample_weight):\n",
        "    model = get_bilstm(train_x.shape[1:], config)\n",
        "    history = model.fit(train_x, train_y, epochs=config.EPOCHS,\n",
        "                        batch_size=config.BATCH_SIZE,\n",
        "                        sample_weight=sample_weight, verbose=1,)\n",
        "    return model, history\n",
        "\n",
        "\n",
        "###############\n",
        "# Predictions #\n",
        "###############\n",
        "\n",
        "\n",
        "def get_bio_predictions(model, x, x_raw, n_classes):\n",
        "    y_hat = model.predict(x)\n",
        "    y_hat = y_hat.reshape(-1, n_classes).argmax(axis=1).reshape(x.shape[:2])\n",
        "    labels = []\n",
        "    for row in x_raw.itertuples():\n",
        "        sent_idx = row.Index - 1\n",
        "        for tok_idx in range(row.n_toks):\n",
        "            if y_hat[sent_idx][tok_idx] == 0:\n",
        "                label = \"O\"\n",
        "            elif y_hat[sent_idx][tok_idx] == 1:\n",
        "                label = \"I\"\n",
        "            else:\n",
        "                label = \"B\"\n",
        "            labels.append(label)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def si_predictions_to_spans(label_df):\n",
        "    spans = []\n",
        "    prev_label = 'O'\n",
        "    prev_span_start = '-1'\n",
        "    prev_span_end = '-1'\n",
        "    prev_article = ''\n",
        "\n",
        "    for row in label_df.itertuples():\n",
        "        article = row.document_id\n",
        "        span_start = row.token_start\n",
        "        span_end = row.token_end\n",
        "        label = row.label\n",
        "\n",
        "        span, prev_span_start = update_prediction(article, label,\n",
        "                                                  span_start, span_end,\n",
        "                                                  prev_article, prev_label,\n",
        "                                                  prev_span_start,\n",
        "                                                  prev_span_end)\n",
        "        if span is not None:\n",
        "            spans.append(span)\n",
        "\n",
        "        prev_article = article\n",
        "        prev_label = label\n",
        "        prev_span_end = span_end\n",
        "\n",
        "    # Make sure we get the last prediction\n",
        "    span, _ = update_prediction(article, label, span_start, span_end,\n",
        "                                prev_article, prev_label, prev_span_start,\n",
        "                                prev_span_end)\n",
        "    if span is not None:\n",
        "        spans.append(span)\n",
        "    return spans\n",
        "\n",
        "\n",
        "# Helper method for si_predictions_to_spans\n",
        "def update_prediction(article, label, span_start, span_end, prev_article,\n",
        "                      prev_label, prev_span_start, prev_span_end):\n",
        "    span = None\n",
        "    cur_span_start = prev_span_start\n",
        "    # Ending a span: I-O, B-O, I-B, B-B, new article\n",
        "    if prev_label != 'O' and (label != 'I' or prev_article != article):\n",
        "        span = (prev_article, prev_span_start, prev_span_end)\n",
        "\n",
        "    # Starting a new span: O-B, O-I, I-B, B-B, new article\n",
        "    if label == 'B' or (label == 'I' and prev_label == 'O') \\\n",
        "            or prev_article != article:\n",
        "        # Update the start of the current label span\n",
        "        cur_span_start = span_start\n",
        "    return span, cur_span_start\n",
        "\n",
        "\n",
        "def print_spans(spans, file_prefix, file_stem, file_suffix):\n",
        "    outfile = file_prefix + 'spans_' + file_stem + '_' + file_suffix + '.txt'\n",
        "    with open(outfile, mode='w') as f:\n",
        "        for span in spans:\n",
        "            f.write(str(span[0]) + '\\t' + str(span[1]) + '\\t' +\n",
        "                    str(span[2]) + '\\n')\n",
        "\n",
        "\n",
        "def predict(config, model, history, dev_df, dev_raw, dev_x, comments,\n",
        "            file_prefix, file_stem, file_suffix, predict_spans=True):\n",
        "    y_hat = get_bio_predictions(model, dev_x, dev_raw, config.N_CLASSES)\n",
        "    result_df = pd.concat([dev_df, pd.DataFrame(y_hat, columns=['label'])],\n",
        "                          axis=1, sort=False)\n",
        "\n",
        "    logfile = file_prefix + 'log_' + file_stem + '_' + file_suffix + '.txt'\n",
        "\n",
        "    with open(logfile, mode='w') as f:\n",
        "        f.write('DATA PREPROCESSING\\n\\n')\n",
        "        for comment in comments:\n",
        "            comment = comment.replace('#', '')\n",
        "            fields = comment.split(',')\n",
        "            for field in fields:\n",
        "                f.write(comment.strip() + '\\n')\n",
        "        f.write('\\n\\nCONFIG\\n\\n')\n",
        "        f.write(config.pretty_str())\n",
        "        f.write('\\n\\nMODEL HISTORY\\n\\n')\n",
        "        f.write('Loss ' + config.LOSS + '\\n')\n",
        "        f.write(str(history.history['loss']) + '\\n')\n",
        "        f.write(config.METRIC + '\\n')\n",
        "        f.write(str(history.history[config.METRIC]) + '\\n')\n",
        "        f.write('\\n\\nMODEL SUMMARY\\n\\n')\n",
        "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "    if predict_spans:\n",
        "        spans = si_predictions_to_spans(result_df)\n",
        "        print_spans(spans, file_prefix, file_stem, file_suffix)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "###########################\n",
        "# Putting it all together #\n",
        "###########################\n",
        "\n",
        "\n",
        "def run(config, file_stem, file_suffix, verbose=True, predict_spans=True,\n",
        "        data=None, word2embedding=None, file_prefix=''):\n",
        "    if verbose:\n",
        "        print('Running with config:')\n",
        "        print(config.pretty_str())\n",
        "    if not data:\n",
        "        if config.LOAD_DATA:\n",
        "            print('Loading data from files')\n",
        "            data = Data(path=config.DATA_PATH)\n",
        "        else:\n",
        "            if verbose:\n",
        "                print('Encoding the data')\n",
        "            data = get_data(config, word2embedding)\n",
        "            if config.SAVE_DATA:\n",
        "                data.save()\n",
        "    if verbose:\n",
        "        print('Building the model')\n",
        "    model, history = create_and_fit_bilstm(config, data.train_x,\n",
        "                                           data.train_y,\n",
        "                                           data.sample_weight)\n",
        "    if verbose:\n",
        "        print('Predicting the test data spans')\n",
        "    labels = predict(config, model, history, data.dev_df, data.dev_raw,\n",
        "                     data.dev_x, data.comments, file_prefix, file_stem,\n",
        "                     file_suffix, predict_spans)\n",
        "    if verbose:\n",
        "        print('Done!\\n\\n')\n",
        "\n",
        "    return data, labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EnkETqKZbqT",
        "colab_type": "text"
      },
      "source": [
        "# grid_search.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAjKAZNfSaWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from model import run, si_predictions_to_spans, print_spans\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, args=None):\n",
        "        \"\"\"Creates a default configuration.\n",
        "\n",
        "        Keyword arguments:\n",
        "        args -- a dict(str -> ?) containing values diverging from the default\n",
        "        \"\"\"\n",
        "        # Encoding the data:\n",
        "        self.MAX_SEQ_LEN = 35\n",
        "        self.EMBED_DIM = 300\n",
        "        self.N_CLASSES = 2\n",
        "        self.ONLINE_SOURCES = True\n",
        "        # train+dev: 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-train%2Bdev.tsv?token=AD7GEDJ7GSTS3RSP5ZSXLZ26LP4BS'\n",
        "        self.TRAIN_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-train.tsv?token=AD7GEDPCTVL5SN46K6LG6EC6LP4BW'\n",
        "        # test: 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-test.tsv?token=AD7GEDM7A3GFIAEZHHESFO26LP4BQ'\n",
        "        self.DEV_URL = 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-dev.tsv?token=AD7GEDLCUDJD7YBIGBJVR526LP4BM'\n",
        "        self.EMBEDDING_PATH = 'gdrive/My Drive/colab_projects/data/glove.42B.300d.zip'  # 'gdrive/My Drive/colab_projects/data/glove.6B.100d.zip'\n",
        "        self.TRAIN_BERT = 'gdrive/My Drive/colab_projects/data/train_bert-base-uncased.tsv'\n",
        "        self.TEST_BERT = 'gdrive/My Drive/colab_projects/data/dev_bert-base-uncased.tsv'\n",
        "        self.USE_BERT = False\n",
        "        self.UNCASED = True  # If true, words are turned into lower case.\n",
        "        self.SAVE_DATA = False  # If true, the following two values can be used\n",
        "                                # for re-using the data next time.\n",
        "        # In case the training & dev data were saved and can be reused:\n",
        "        self.DATA_PATH = 'gdrive/My Drive/colab_projects/data/data/'\n",
        "        self.LOAD_DATA = False\n",
        "\n",
        "        # Building the model:\n",
        "        self.BATCH_SIZE = 128\n",
        "        self.EPOCHS = 10\n",
        "        self.O_WEIGHT = 1.0\n",
        "        self.I_WEIGHT = 6.5\n",
        "        self.B_WEIGHT = 6.5\n",
        "        self.LSTM_UNITS = 512\n",
        "        self.DROPOUT = 0.25\n",
        "        self.OPTIMIZER = 'adam'\n",
        "        self.METRIC = 'categorical_accuracy'\n",
        "        self.LOSS = 'categorical_crossentropy'\n",
        "\n",
        "        # Making predictions:\n",
        "        self.MAJORITY_VOTING = True\n",
        "\n",
        "        if args:\n",
        "            for key in args:\n",
        "                setattr(self, key, args[key])\n",
        "\n",
        "        if self.USE_BERT:\n",
        "            self.EMBED_DIM = 768\n",
        "\n",
        "    def pretty_str(self):\n",
        "        return 'max seq len: ' + str(self.MAX_SEQ_LEN) + '\\n' + \\\n",
        "               'embedding depth: ' + str(self.EMBED_DIM) + '\\n' + \\\n",
        "               'BERT embeddings: ' + str(self.USE_BERT) + '\\n' + \\\n",
        "               'TRAIN_BERT: ' + str(self.TRAIN_BERT) + '\\n' + \\\n",
        "               'DEV_BERT: ' + str(self.DEV_BERT) + '\\n' + \\\n",
        "               'number of labels: ' + str(config.N_CLASSES) + '\\n' + \\\n",
        "               'batch size: ' + str(self.BATCH_SIZE) + '\\n' + \\\n",
        "               'epochs: ' + str(self.EPOCHS) + '\\n' + \\\n",
        "               'O weight: ' + str(self.O_WEIGHT) + \\\n",
        "               ', I weight:' + str(self.I_WEIGHT) + \\\n",
        "               ', B weight: ' + str(self.B_WEIGHT) + '\\n' + \\\n",
        "               'hidden units: ' + str(self.LSTM_UNITS) + '\\n' + \\\n",
        "               'dropout rate: ' + str(self.DROPOUT) + '\\n' + \\\n",
        "               'optimizer: ' + self.OPTIMIZER + '\\n' + \\\n",
        "               'metric: ' + self.METRIC + '\\n' + \\\n",
        "               'loss: ' + self.LOSS + '\\n'\n",
        "\n",
        "\n",
        "def get_majority_vote(votes):\n",
        "    votes = dict(Counter(votes))\n",
        "    max_count = -1\n",
        "    max_entry = []\n",
        "    for key in votes:\n",
        "        count = votes[key]\n",
        "        if count > max_count:\n",
        "            max_count = count\n",
        "            max_entry = [key]\n",
        "        elif count == max_count:\n",
        "            max_entry.append(key)\n",
        "    # For our data, preferring specific labels in tie situations actually\n",
        "    # doesn't make a difference.\n",
        "    return max_entry[0]\n",
        "\n",
        "\n",
        "def run_config(config, file_prefix, data=None, repetitions=5, verbose=True):\n",
        "    now = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
        "    predictions = None\n",
        "    label_cols = []\n",
        "    for i in range(repetitions):\n",
        "        if verbose:\n",
        "            print(\"Iteration \" + str(i + 1) + \" of \" + str(repetitions))\n",
        "        data, labels = run(config, data=data, verbose=verbose,\n",
        "                           file_prefix=file_prefix, file_stem=now,\n",
        "                           file_suffix=str(i))\n",
        "        if config.MAJORITY_VOTING:\n",
        "            if predictions is None:\n",
        "                predictions = labels\n",
        "                predictions = predictions.rename(columns={'label': 'label_0'})\n",
        "            else:\n",
        "                predictions.insert(loc=len(predictions.columns),\n",
        "                                   column='label_' + str(i),\n",
        "                                   value=labels.label)\n",
        "            label_cols.append('label_' + str(i))\n",
        "    if config.MAJORITY_VOTING:\n",
        "        labels = []\n",
        "        for row in predictions.itertuples():\n",
        "            labels.append(get_majority_vote(\n",
        "                [getattr(row, l) for l in label_cols]))\n",
        "        predictions['label'] = labels\n",
        "        spans = si_predictions_to_spans(predictions)\n",
        "        print_spans(spans, file_prefix, now, 'majority')\n",
        "\n",
        "    # Return data in case the next config only changes model features\n",
        "    return data\n",
        "\n",
        "\n",
        "file_prefix = '/content/gdrive/My Drive/colab_projects/semeval-predictions/'\n",
        "data = None\n",
        "\n",
        "## Hyperparameter tuning:\n",
        "# for epochs in [5, 10, 15]:\n",
        "#     for dropout in [0.2, 0.4, 0.6, 0.8]:\n",
        "#         config = Config({'EPOCHS': epochs, 'DROPOUT': dropout})\n",
        "#         data = run_config(config, file_prefix, data)\n",
        "\n",
        "## You can change config values by passing a dictionary to the constructor:\n",
        "# config = Config({'LOAD_DATA': True})\n",
        "config = Config({'USE_BERT': True})\n",
        "\n",
        "## For predictions on the final test set:\n",
        "# config = Config({'USE_BERT': True,\n",
        "#                  'TRAIN_URL': 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-train%2Bdev.tsv?token=AD7GEDJ7GSTS3RSP5ZSXLZ26LP4BS',\n",
        "#                  'DEV_URL': 'https://raw.githubusercontent.com/cicl-iscl/CyberWallE/master/data/si-test.tsv?token=AD7GEDM7A3GFIAEZHHESFO26LP4BQ'\n",
        "#                  })\n",
        "\n",
        "data = run_config(config, file_prefix, data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}